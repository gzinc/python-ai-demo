# Module 1: NumPy Basics

**Duration**: 3-4 days
**Goal**: Master NumPy arrays and vectorized operations

## ğŸ¯ Why NumPy Matters for AI

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     NUMPY: THE FOUNDATION OF AI                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   "Hello world"          Embedding Model           NumPy Array              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚    TEXT     â”‚ â”€â”€â”€â”€â”€â–º â”‚ text-embed  â”‚ â”€â”€â”€â”€â”€â–º  â”‚ [0.23, -0.54, 0.12, â”‚   â”‚
â”‚   â”‚             â”‚        â”‚ -ada-002    â”‚         â”‚  0.87, -0.33, ...]  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚     768 dimensions  â”‚   â”‚
â”‚                                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚               â”‚
â”‚                                                             â–¼               â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚                      AI/ML APPLICATIONS                              â”‚  â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚  â”‚
â”‚   â”‚  â”‚ RAG Systems  â”‚  â”‚   Search     â”‚  â”‚  Clustering  â”‚              â”‚  â”‚
â”‚   â”‚  â”‚ (similarity) â”‚  â”‚  (nearest)   â”‚  â”‚  (k-means)   â”‚              â”‚  â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   ğŸ’¡ INSIGHT: Every text embedding IS a NumPy array!                       â”‚
â”‚   â†’ NumPy skills = Embedding manipulation skills                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Embeddings**: Text â†’ vectors (NumPy arrays)
- **Model inputs**: All ML models take NumPy arrays
- **Performance**: 10-100x faster than Python loops
- **Foundation**: TensorFlow, PyTorch build on NumPy concepts

## ğŸ“š Topics Covered

### 1. Array Creation
```python
import numpy as np

# From lists
arr = np.array([1, 2, 3, 4, 5])

# Common patterns
zeros = np.zeros((3, 4))        # 3x4 array of zeros
ones = np.ones((2, 3))          # 2x3 array of ones
identity = np.eye(4)            # 4x4 identity matrix
random = np.random.rand(3, 3)   # 3x3 random values [0, 1)

# Ranges
sequence = np.arange(0, 10, 2)  # [0, 2, 4, 6, 8]
linspace = np.linspace(0, 1, 5) # 5 values from 0 to 1
```

### 2. Array Operations
```python
# Element-wise operations (vectorized!)
arr1 = np.array([1, 2, 3])
arr2 = np.array([4, 5, 6])

result = arr1 + arr2  # [5, 7, 9]
result = arr1 * 2     # [2, 4, 6]
result = arr1 ** 2    # [1, 4, 9]

# Math functions
np.sqrt(arr1)
np.exp(arr1)
np.log(arr1)
```

### 3. Indexing and Slicing
```python
arr = np.array([10, 20, 30, 40, 50])

# Basic indexing
arr[0]      # 10
arr[-1]     # 50 (last element)

# Slicing
arr[1:4]    # [20, 30, 40]
arr[:3]     # [10, 20, 30]
arr[::2]    # [10, 30, 50] (every 2nd element)

# 2D arrays
matrix = np.array([[1, 2, 3],
                   [4, 5, 6],
                   [7, 8, 9]])

matrix[0, 1]    # 2 (row 0, col 1)
matrix[:, 1]    # [2, 5, 8] (all rows, col 1)
matrix[1, :]    # [4, 5, 6] (row 1, all cols)
```

### 4. Broadcasting
**The secret to efficient AI code!**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BROADCASTING VISUALIZATION                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   arr = [1, 2, 3]         matrix = [[1],       arr + matrix = ?             â”‚
â”‚   Shape: (3,)                       [2],       Shapes don't match!          â”‚
â”‚                                     [3]]       But NumPy handles it...      â”‚
â”‚                            Shape: (3, 1)                                    â”‚
â”‚                                                                             â”‚
â”‚   STEP 1: NumPy "stretches" both arrays to match:                          â”‚
â”‚                                                                             â”‚
â”‚   arr broadcasts â†’         matrix broadcasts â†“                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚   â”‚ [1, 2, 3]         â”‚    â”‚ [1, 1, 1]         â”‚                           â”‚
â”‚   â”‚ [1, 2, 3]  â†copy  â”‚    â”‚ [2, 2, 2]  â†copy  â”‚                           â”‚
â”‚   â”‚ [1, 2, 3]  â†copy  â”‚    â”‚ [3, 3, 3]  â†copy  â”‚                           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚         (3, 3)                   (3, 3)                                     â”‚
â”‚                                                                             â”‚
â”‚   STEP 2: Element-wise addition:                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚ [1, 2, 3]         â”‚    â”‚ [1, 1, 1]         â”‚    â”‚ [2, 3, 4]         â”‚  â”‚
â”‚   â”‚ [1, 2, 3]         â”‚ +  â”‚ [2, 2, 2]         â”‚ =  â”‚ [3, 4, 5]         â”‚  â”‚
â”‚   â”‚ [1, 2, 3]         â”‚    â”‚ [3, 3, 3]         â”‚    â”‚ [4, 5, 6]         â”‚  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                             â”‚
â”‚   ğŸ’¡ WHY THIS MATTERS FOR AI:                                              â”‚
â”‚   - Normalize 100 embeddings at once (no loops!)                           â”‚
â”‚   - Compute similarity between query and all documents                     â”‚
â”‚   - Apply transformations to batches efficiently                           â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# Different shapes work together
arr = np.array([1, 2, 3])
matrix = np.array([[1],
                   [2],
                   [3]])

# Broadcasting automatically aligns dimensions
result = arr + matrix
# [[2, 3, 4],
#  [3, 4, 5],
#  [4, 5, 6]]

# Real AI example: normalize embeddings
embeddings = np.random.rand(100, 768)  # 100 texts, 768 dimensions
mean = embeddings.mean(axis=1, keepdims=True)
std = embeddings.std(axis=1, keepdims=True)
normalized = (embeddings - mean) / std  # Broadcasting!
```

### 5. Reshaping
```python
arr = np.arange(12)  # [0, 1, 2, ..., 11]

# Reshape
matrix = arr.reshape(3, 4)  # 3 rows, 4 cols
cube = arr.reshape(2, 2, 3)  # 3D array

# Flatten
flat = matrix.flatten()  # Back to 1D

# Transpose
transposed = matrix.T  # Swap rows and columns
```

## ğŸš€ Hands-On Examples

Run these examples:

```bash
# Interactive exploration
uv run python examples.py

# Exercises (try first!)
uv run python exercises.py

# Check solutions
uv run python solutions/exercise_solutions.py
```

## ğŸ“ AI Application Examples

### Example 1: Similarity Between Vectors (Embeddings)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     COSINE SIMILARITY IN RAG SYSTEMS                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Query: "How do I use Python?"       Document: "Python programming basics" â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚          â”‚ embedding   â”‚                       â”‚ embedding   â”‚              â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                 â”‚                                     â”‚                     â”‚
â”‚                 â–¼                                     â–¼                     â”‚
â”‚   Query vec: [0.2, 0.5, 0.8]         Doc vec: [0.3, 0.6, 0.7]             â”‚
â”‚                                                                             â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚         COSINE SIMILARITY               â”‚                   â”‚
â”‚              â”‚                                         â”‚                   â”‚
â”‚              â”‚      dot(query, doc)                    â”‚                   â”‚
â”‚              â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”‚                   â”‚
â”‚              â”‚  ||query|| Ã— ||doc||                    â”‚                   â”‚
â”‚              â”‚                                         â”‚                   â”‚
â”‚              â”‚  = (0.2Ã—0.3 + 0.5Ã—0.6 + 0.8Ã—0.7)       â”‚                   â”‚
â”‚              â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚                   â”‚
â”‚              â”‚    sqrt(0.93) Ã— sqrt(0.94)              â”‚                   â”‚
â”‚              â”‚                                         â”‚                   â”‚
â”‚              â”‚  = 0.92 / 0.935 = 0.984 âœ“              â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                 â”‚                                          â”‚
â”‚                                 â–¼                                          â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚              â”‚  Similarity Scale:                      â”‚                   â”‚
â”‚              â”‚  0.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.0   â”‚                   â”‚
â”‚              â”‚  Different â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Same     â”‚                   â”‚
â”‚              â”‚                          0.984 âœ“        â”‚                   â”‚
â”‚              â”‚                     (Very Similar!)     â”‚                   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```python
# Simulating text embeddings
text1_embedding = np.array([0.2, 0.5, 0.8])
text2_embedding = np.array([0.3, 0.6, 0.7])

# Cosine similarity (used in RAG systems!)
def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

similarity = cosine_similarity(text1_embedding, text2_embedding)
print(f"Similarity: {similarity:.4f}")  # 0.0 = different, 1.0 = identical
```

### Example 2: Batch Processing (like LLM token processing)
```python
# Process multiple texts at once
batch_embeddings = np.random.rand(32, 768)  # 32 texts, 768 dims

# Normalize all at once (no loops!)
norms = np.linalg.norm(batch_embeddings, axis=1, keepdims=True)
normalized_batch = batch_embeddings / norms
```

### Example 3: Attention Scores (simplified transformer concept)
```python
# Simplified attention mechanism
query = np.array([1.0, 0.5, 0.2])
keys = np.array([
    [1.0, 0.3, 0.1],
    [0.5, 0.9, 0.4],
    [0.2, 0.1, 0.8]
])

# Compute attention scores
scores = np.dot(keys, query)  # How relevant each key is to query
attention_weights = np.exp(scores) / np.sum(np.exp(scores))  # Softmax

print(f"Attention weights: {attention_weights}")
# Shows which keys are most relevant
```

## ğŸ“ Exercises

### Exercise 1: Array Manipulation
Create a function that:
1. Generates a random 5x5 matrix
2. Normalizes each row (subtract mean, divide by std)
3. Returns the normalized matrix

### Exercise 2: Vector Operations
Implement these functions:
- `euclidean_distance(a, b)` - Distance between vectors
- `cosine_similarity(a, b)` - Similarity score
- `find_nearest(query, vectors)` - Find most similar vector

### Exercise 3: Performance Comparison
Compare loop vs vectorization:
```python
# Loop version
def add_loop(arr1, arr2):
    result = []
    for i in range(len(arr1)):
        result.append(arr1[i] + arr2[i])
    return result

# Vectorized version
def add_vectorized(arr1, arr2):
    return arr1 + arr2

# Time both with large arrays
```

## ğŸ¯ Key Takeaways

1. **No Loops**: Use vectorized operations whenever possible
2. **Broadcasting**: Let NumPy handle dimension alignment
3. **Memory**: NumPy arrays are more efficient than Python lists
4. **Foundation**: These concepts apply to all AI/ML work

## âœ… Completion Checklist

- [ ] Understand array creation methods
- [ ] Comfortable with indexing and slicing
- [ ] Can explain broadcasting
- [ ] Know when to use reshape vs transpose
- [ ] Completed all exercises
- [ ] Built similarity search example
- [ ] Documented learnings in memory

## ğŸ”„ Next: Pandas Data Manipulation

Once comfortable with NumPy:
- Move to [02_pandas_basics/](../02_pandas_basics/)
- DataFrames build on NumPy arrays
- Learn data manipulation for LLM applications

---

**Practice makes perfect! Spend time experimenting. ğŸš€**
