"""
LangChain Prompts - Practical Examples with Real LLM Integration

This module demonstrates LangChain prompt templates with actual LLM API calls.
Requires OPENAI_API_KEY to be set in .env file.

Run: uv run python -m phase7_frameworks.01_langchain_basics.01_prompts.practical

Requires: OPENAI_API_KEY in .env
"""

from inspect import cleandoc
from typing import TYPE_CHECKING

from langchain_core.prompts import (
    ChatPromptTemplate,
    FewShotChatMessagePromptTemplate,
    FewShotPromptTemplate,
    MessagesPlaceholder,
    PromptTemplate,
)
from langchain_core.messages import AIMessage, HumanMessage

from phase7_frameworks.utils import check_api_keys, print_section, requires_openai

if TYPE_CHECKING:
    from langchain_openai import ChatOpenAI


# region Helper Functions


def print_subsection(title: str) -> None:
    """print subsection header"""
    print(f"\n{'-' * 70}")
    print(f"  {title}")
    print('-' * 70)


def print_llm_output(label: str, response: str) -> None:
    """print LLM response with formatting"""
    print(f"\n{label}:")
    print(f"  {response}")


def get_llm(temperature: float = 0.7) -> "ChatOpenAI":
    """create configured LLM instance"""
    from langchain_openai import ChatOpenAI

    return ChatOpenAI(
        model="gpt-4o-mini",
        temperature=temperature,
    )


# endregion


# region 1. PromptTemplate with LLM


def demo_prompt_template_with_llm() -> None:
    """
    demonstrate PromptTemplate with actual LLM calls

    LCEL Pattern: Template | LLM
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         PromptTemplate with LLM Integration                 ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  1. Variable Input:                                         ‚îÇ
    ‚îÇ     {topic: "embeddings", style: "simple"}                  ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  2. PromptTemplate:                                         ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ "Explain {topic} in {style} terms,   ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  using only 2 sentences."            ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ     Formatted: "Explain embeddings in simple terms,         ‚îÇ
    ‚îÇ                 using only 2 sentences."                    ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  3. LLM (gpt-4o-mini):                                      ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ  Processes formatted prompt          ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  Temperature: 0.3 (focused)          ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  4. AIMessage Response:                                     ‚îÇ
    ‚îÇ     "Embeddings are numerical representations..."           ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  LCEL Syntax: chain = template | llm                        ‚îÇ
    ‚îÇ              response = chain.invoke(params)                ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Clean composition with pipe operator           ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Automatic prompt formatting                    ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Type-safe variable substitution                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("1. PromptTemplate with Real LLM Integration")

    print_subsection("Simple Template -> LLM")

    # Create template
    template = PromptTemplate.from_template(
        "Explain {topic} in {style} terms, using only 2 sentences."
    )

    # Create LLM
    llm = get_llm(temperature=0.3)

    # Use with LCEL pipe operator
    chain = template | llm

    # Execute
    topics = [
        {"topic": "embeddings", "style": "simple"},
        {"topic": "vector databases", "style": "technical"},
        {"topic": "RAG systems", "style": "business"},
    ]

    for params in topics:
        print(f"\n{'‚îÄ' * 70}")
        print(f"Topic: {params['topic']} | Style: {params['style']}")
        response = chain.invoke(params)
        print_llm_output("LLM Response", response.content)

    print_subsection("Key Pattern: LCEL Composition")
    explanation = cleandoc('''
        template | llm

        ‚úÖ Pipe operator (|) chains components
        ‚úÖ Template formats prompt
        ‚úÖ LLM processes formatted prompt
        ‚úÖ Returns AIMessage with response
    ''')
    print(f"\n{explanation}")


# endregion


# region 2. ChatPromptTemplate with LLM


def demo_chat_template_with_llm() -> None:
    """
    demonstrate ChatPromptTemplate with actual LLM calls

    Multi-Message Template Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       ChatPromptTemplate: Multi-Message Conversations       ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  1. Input Variables:                                        ‚îÇ
    ‚îÇ    {domain: "machine learning", concept: "gradient descent"}‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  2. ChatPromptTemplate.from_messages():                     ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ ("system", "You are an expert in     ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  {domain}. Provide concise answers") ‚îÇ                ‚îÇ
    ‚îÇ     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                ‚îÇ
    ‚îÇ     ‚îÇ ("human", "Explain {concept} in      ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  2-3 sentences.")                    ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ     Formatted Messages:                                     ‚îÇ
    ‚îÇ     [SystemMessage: "You are an expert in machine learning"]‚îÇ
    ‚îÇ     [HumanMessage: "Explain gradient descent in 2-3..."]    ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  3. LLM Processing:                                         ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ  Chat model processes message list   ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  System message sets behavior        ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  Human message defines task          ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  4. AIMessage Response:                                     ‚îÇ
    ‚îÇ     Expert-level explanation following system instruction   ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: System message customizes LLM behavior         ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Supports multi-turn conversations              ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Clean separation of role-based messages        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

    Few-Shot Pattern (Add Examples):
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                Teaching Response Style                      ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Template Structure:                                        ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
    ‚îÇ  ‚îÇ ("system", "You are an educator...")   ‚îÇ                 ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                 ‚îÇ
    ‚îÇ  ‚îÇ ("human", "What is a cache?")          ‚îÇ  ‚Üê Example 1    ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                 ‚îÇ
    ‚îÇ  ‚îÇ ("ai", "A cache is like a desk...")    ‚îÇ  ‚Üê Response     ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§     style       ‚îÇ
    ‚îÇ  ‚îÇ ("human", "What is {concept}?")        ‚îÇ  ‚Üê User input   ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  LLM learns tone, structure, and formatting from examples   ‚îÇ
    ‚îÇ  ‚Üí Applies same style to new questions                      ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Consistent response formatting                 ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Easy to demonstrate desired tone               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("2. ChatPromptTemplate with Real LLM Integration")

    print_subsection("Multi-Message Template -> LLM")

    # Create chat template
    template = ChatPromptTemplate.from_messages([
        ("system", "You are an expert in {domain}. Always provide concise, accurate answers."),
        ("human", "Explain {concept} in 2-3 sentences."),
    ])

    # Create chain
    llm = get_llm(temperature=0.3)
    chain = template | llm

    # Execute with different domains
    queries = [
        {"domain": "machine learning", "concept": "gradient descent"},
        {"domain": "distributed systems", "concept": "eventual consistency"},
        {"domain": "frontend development", "concept": "virtual DOM"},
    ]

    for params in queries:
        print(f"\n{'‚îÄ' * 70}")
        print(f"Domain: {params['domain']}")
        print(f"Concept: {params['concept']}")
        response = chain.invoke(params)
        print_llm_output("LLM Response", response.content)

    print_subsection("Adding Few-Shot Examples")

    # Template with examples
    expert_template = ChatPromptTemplate.from_messages([
        ("system", "You are a technical educator. Explain concepts with analogies."),
        ("human", "What is a cache?"),
        ("ai", "A cache is like a desk drawer. Instead of walking to the filing cabinet (slow storage) every time, you keep frequently used items in your drawer (fast cache) for quick access."),
        ("human", "What is {concept}?"),
    ])

    chain = expert_template | llm

    response = chain.invoke({"concept": "load balancer"})
    print(f"\n{'‚îÄ' * 70}")
    print("Concept: load balancer")
    print_llm_output("LLM Response with Style", response.content)


# endregion


# region 3. MessagesPlaceholder with Chat History


def demo_messages_placeholder_with_llm() -> None:
    """
    demonstrate MessagesPlaceholder for chat memory

    Chat History Management Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ        MessagesPlaceholder: Conversational Context          ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Template Structure:                                        ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
    ‚îÇ  ‚îÇ ("system", "You are an AI assistant")  ‚îÇ                 ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§                 ‚îÇ
    ‚îÇ  ‚îÇ MessagesPlaceholder("chat_history")    ‚îÇ ‚Üê Expands to    ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§   full history  ‚îÇ
    ‚îÇ  ‚îÇ ("human", "{question}")                ‚îÇ                 ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Conversation Flow:                                         ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
    ‚îÇ  ‚îÇ Turn 1: "What is RAG?"                       ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ   chat_history = []                          ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ   LLM Response ‚Üí Added to history            ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ                                              ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ Turn 2: "Can you give me an example?"        ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ   chat_history = [                           ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ     HumanMessage("What is RAG?"),            ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ     AIMessage("RAG is...")                   ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ   ]                                          ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ   LLM sees full context ‚Üí coherent response  ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ                                              ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ Turn 3: "What embedding model for that?"     ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ   chat_history = [Turn 1 + Turn 2]           ‚îÇ           ‚îÇ
    ‚îÇ  ‚îÇ   LLM understands "that" refers to RAG       ‚îÇ           ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Automatic context injection                    ‚îÇ
    ‚îÇ  ‚úÖ Benefit: No manual prompt engineering                   ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Foundation for chat memory systems             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Supports arbitrarily long conversations        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("3. MessagesPlaceholder with Chat History")

    print_subsection("Building Conversational Context")

    # Create template with history placeholder
    template = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful AI assistant. Keep responses brief."),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ])

    # Create chain
    llm = get_llm(temperature=0.7)
    chain = template | llm

    # Simulate conversation
    chat_history = []

    print("\nConversation:")

    # Turn 1
    print(f"\n{'‚îÄ' * 70}")
    print("User: What is RAG?")
    response = chain.invoke({
        "chat_history": chat_history,
        "question": "What is RAG?"
    })
    print_llm_output("AI", response.content)

    # Update history
    chat_history.extend([
        HumanMessage(content="What is RAG?"),
        AIMessage(content=response.content),
    ])

    # Turn 2 (references previous context)
    print(f"\n{'‚îÄ' * 70}")
    print("User: Can you give me an example?")
    response = chain.invoke({
        "chat_history": chat_history,
        "question": "Can you give me an example?"
    })
    print_llm_output("AI", response.content)

    # Update history
    chat_history.extend([
        HumanMessage(content="Can you give me an example?"),
        AIMessage(content=response.content),
    ])

    # Turn 3 (continues context)
    print(f"\n{'‚îÄ' * 70}")
    print("User: What embedding model should I use for that?")
    response = chain.invoke({
        "chat_history": chat_history,
        "question": "What embedding model should I use for that?"
    })
    print_llm_output("AI", response.content)

    print_subsection("Why This Works")
    explanation = cleandoc('''
        ‚úÖ MessagesPlaceholder injects full conversation history
        ‚úÖ LLM sees all previous context for coherent responses
        ‚úÖ No manual prompt engineering for context
        ‚úÖ Foundation for chat memory systems
    ''')
    print(f"\n{explanation}")


# endregion


# region 4. FewShotPromptTemplate with LLM


def demo_few_shot_with_llm() -> None:
    """
    demonstrate FewShotPromptTemplate for in-context learning

    Few-Shot Learning Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ        FewShotPromptTemplate: In-Context Learning           ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Prompt Construction:                                       ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
    ‚îÇ  ‚îÇ PREFIX:                                        ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ "Classify sentiment as positive/negative..."   ‚îÇ         ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îÇ
    ‚îÇ  ‚îÇ EXAMPLES (teach format):                       ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Text: "I loved this!"                        ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Sentiment: positive                          ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Text: "Terrible quality"                     ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Sentiment: negative                          ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Text: "It's okay"                            ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Sentiment: neutral                           ‚îÇ         ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îÇ
    ‚îÇ  ‚îÇ SUFFIX (new input):                            ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Text: {input}                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Sentiment:                                   ‚îÇ         ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  LLM learns pattern from examples                           ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  Consistent Output: "positive" | "negative" | "neutral"     ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Benefits of Few-Shot Learning:                             ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
    ‚îÇ  ‚îÇ ‚úÖ No fine-tuning required                     ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ ‚úÖ Consistent output structure                 ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ ‚úÖ Easy to update examples                     ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ ‚úÖ Works with any classification task          ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ ‚úÖ Temperature=0.0 for reproducibility         ‚îÇ         ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Use Cases:                                                 ‚îÇ
    ‚îÇ  ‚Ä¢ Sentiment analysis ‚Ä¢ Text classification                 ‚îÇ
    ‚îÇ  ‚Ä¢ Entity extraction ‚Ä¢ Format standardization               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("4. FewShotPromptTemplate with In-Context Learning")

    print_subsection("Teaching Task Format Through Examples")

    # Define examples
    examples = [
        {
            "input": "I absolutely loved this product! Best purchase ever!",
            "output": "positive",
        },
        {
            "input": "Terrible quality. Waste of money.",
            "output": "negative",
        },
        {
            "input": "It's okay. Does what it says, nothing special.",
            "output": "neutral",
        },
    ]

    # Create example template
    example_template = PromptTemplate(
        input_variables=["input", "output"],
        template="Text: {input}\nSentiment: {output}",
    )

    # Create few-shot template
    few_shot_template = FewShotPromptTemplate(
        examples=examples,
        example_prompt=example_template,
        prefix="Classify the sentiment of the following text as positive, negative, or neutral:",
        suffix="Text: {input}\nSentiment:",
        input_variables=["input"],
    )

    # Create chain
    llm = get_llm(temperature=0.0)  # Low temperature for consistency
    chain = few_shot_template | llm

    # Test with new inputs
    test_texts = [
        "This exceeded my expectations! Highly recommend!",
        "Not worth the price. Very disappointed.",
        "It works as advertised. Nothing more, nothing less.",
        "Amazing! Would buy again in a heartbeat!",
    ]

    print("\nClassification Results:")
    for text in test_texts:
        response = chain.invoke({"input": text})
        # Extract just the sentiment (first word of response)
        sentiment = response.content.strip().split()[0]
        print(f"\n  Text: {text}")
        print(f"  Sentiment: {sentiment}")

    print_subsection("Why Few-Shot Works")
    explanation = cleandoc('''
        ‚úÖ LLM learns format from examples
        ‚úÖ Consistent output structure
        ‚úÖ No fine-tuning required
        ‚úÖ Easy to update examples
        ‚úÖ Works with any classification task
    ''')
    print(f"\n{explanation}")


# endregion


# region 5. FewShotChatMessagePromptTemplate


def demo_few_shot_chat_with_llm() -> None:
    """
    demonstrate FewShotChatMessagePromptTemplate for response style

    Teaching Response Style with Chat Examples:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     FewShotChatMessagePromptTemplate: Style Learning        ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Full Template Structure:                                   ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
    ‚îÇ  ‚îÇ SYSTEM MESSAGE:                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ "You are a technical educator.                 ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ  Follow response style from examples."         ‚îÇ         ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îÇ
    ‚îÇ  ‚îÇ EXAMPLE 1:                                     ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Human: "What is a REST API?"                 ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   AI: "REST API is an interface...             ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ        üîë Key Points:                         ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ        ‚Ä¢ Uses HTTP methods                     ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ        ‚Ä¢ Stateless communication               ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ        üí° Example: GET /users/123"             ‚îÇ         ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îÇ
    ‚îÇ  ‚îÇ EXAMPLE 2:                                     ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Human: "What is GraphQL?"                    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   AI: "GraphQL is a query language...          ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ        üîë Key Points:                          ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ        ‚Ä¢ Single endpoint                       ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ        ...same structure..."                   ‚îÇ         ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§         ‚îÇ
    ‚îÇ  ‚îÇ USER INPUT:                                    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   Human: "{input}"                             ‚îÇ         ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  LLM Pattern Learning:                                      ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
    ‚îÇ  ‚îÇ 1. Observe structure across examples           ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ 2. Extract tone and formatting patterns        ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ 3. Apply to new question                       ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ 4. Maintain consistent emoji usage             ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ 5. Follow example organization                 ‚îÇ         ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  Styled Response matching example format                    ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Consistent branded responses                   ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Easy to update style                           ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Educational content standardization            ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Works for any response format                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("5. FewShotChatMessagePromptTemplate for Response Style")

    print_subsection("Teaching Response Pattern Through Examples")

    # Define example conversations
    examples = [
        {
            "input": "What is a REST API?",
            "output": cleandoc('''
                A REST API is an interface for communication between systems.

                üîë Key Points:
                ‚Ä¢ Uses HTTP methods (GET, POST, PUT, DELETE)
                ‚Ä¢ Stateless communication
                ‚Ä¢ Resource-based URLs

                üí° Example: GET /users/123 retrieves user #123
            '''),
        },
        {
            "input": "What is GraphQL?",
            "output": cleandoc('''
                GraphQL is a query language for APIs that lets clients request exactly the data they need.

                üîë Key Points:
                ‚Ä¢ Single endpoint for all queries
                ‚Ä¢ Client-defined response structure
                ‚Ä¢ Reduces over-fetching

                üí° Example: Query { user(id: 123) { name, email } }
            '''),
        },
    ]

    # Create example template
    example_template = ChatPromptTemplate.from_messages([
        ("human", "{input}"),
        ("ai", "{output}"),
    ])

    # Create few-shot chat template
    few_shot_template = FewShotChatMessagePromptTemplate(
        examples=examples,
        example_prompt=example_template,
    )

    # Create full template
    full_template = ChatPromptTemplate.from_messages([
        ("system", "You are a technical educator. Follow the response style shown in examples."),
        few_shot_template,
        ("human", "{input}"),
    ])

    # Create chain
    llm = get_llm(temperature=0.7)
    chain = full_template | llm

    # Test with new question
    print("\nTeaching Response Style:")
    print(f"\n{'‚îÄ' * 70}")
    print("Question: What is a WebSocket?")

    response = chain.invoke({"input": "What is a WebSocket?"})
    print_llm_output("Styled Response", response.content)

    print_subsection("Why This Pattern Works")
    explanation = cleandoc('''
        ‚úÖ LLM learns tone and structure from examples
        ‚úÖ Consistent formatting across responses
        ‚úÖ Easy to update style by changing examples
        ‚úÖ Perfect for branded or educational content
    ''')
    print(f"\n{explanation}")


# endregion


# region 6. Output Parsers


def demo_output_parsers() -> None:
    """
    demonstrate output parsers for structured responses

    Output Parser Pipeline:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ           Output Parsers: String ‚Üí Structured Data          ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Three Parser Types Demonstrated:                           ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  1. StrOutputParser (String):                               ‚îÇ
    ‚îÇ     Template | LLM | StrOutputParser()                      ‚îÇ
    ‚îÇ        ‚îÇ       ‚îÇ           ‚îÇ                                ‚îÇ
    ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ
    ‚îÇ                 ‚îÇ                                           ‚îÇ
    ‚îÇ                 ‚ñº                                           ‚îÇ
    ‚îÇ     AIMessage.content ‚Üí str (default extraction)            ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  2. CommaSeparatedListOutputParser (List):                  ‚îÇ
    ‚îÇ     Template | LLM | CommaSeparatedListOutputParser()       ‚îÇ
    ‚îÇ        ‚îÇ       ‚îÇ           ‚îÇ                                ‚îÇ
    ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ
    ‚îÇ                 ‚îÇ                                           ‚îÇ
    ‚îÇ                 ‚ñº                                           ‚îÇ
    ‚îÇ     "Python, Java, Go" ‚Üí ["Python", "Java", "Go"]           ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  3. JsonOutputParser (Structured):                          ‚îÇ
    ‚îÇ     Template | LLM | JsonOutputParser(pydantic_object)      ‚îÇ
    ‚îÇ        ‚îÇ       ‚îÇ           ‚îÇ                                ‚îÇ
    ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                ‚îÇ
    ‚îÇ                 ‚îÇ                                           ‚îÇ
    ‚îÇ                 ‚ñº                                           ‚îÇ
    ‚îÇ     JSON string ‚Üí Dict[str, Any]                            ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
    ‚îÇ  ‚îÇ Pydantic Schema Example:                    ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ                                             ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ class TechStack(BaseModel):                 ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ     frontend: str                           ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ     backend: str                            ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ     database: str                           ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ     reason: str                             ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ                                             ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ Parser validates against schema             ‚îÇ            ‚îÇ
    ‚îÇ  ‚îÇ Returns typed dict                          ‚îÇ            ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Benefits of Output Parsers:                                ‚îÇ
    ‚îÇ  ‚úÖ Type Safety: Structured data instead of strings         ‚îÇ
    ‚îÇ  ‚úÖ Validation: Ensure LLM follows schema                   ‚îÇ
    ‚îÇ  ‚úÖ Integration: Easy to use in application code            ‚îÇ
    ‚îÇ  ‚úÖ Error Handling: Catch malformed responses early         ‚îÇ
    ‚îÇ  ‚úÖ Documentation: Schema serves as API contract            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("6. Output Parsers for Structured Data")

    print_subsection("String Output Parser (Default)")

    from langchain_core.output_parsers import StrOutputParser

    template = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant."),
        ("human", "List 3 benefits of {topic} (one per line, no numbering)"),
    ])

    llm = get_llm(temperature=0.3)
    chain = template | llm | StrOutputParser()

    response = chain.invoke({"topic": "vector databases"})
    print_llm_output("String Output", response)

    print_subsection("Comma-Separated List Parser")

    from langchain_core.output_parsers import CommaSeparatedListOutputParser

    list_parser = CommaSeparatedListOutputParser()

    template = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant."),
        ("human", "List 5 programming languages suitable for AI development"),
        ("human", "Format: comma-separated list only, no explanations"),
    ])

    chain = template | llm | list_parser

    languages = chain.invoke({})
    print(f"\nParsed List: {languages}")
    print(f"Type: {type(languages)}")

    print_subsection("Structured Output with JSON")

    from langchain_core.output_parsers import JsonOutputParser
    from langchain_core.pydantic_v1 import BaseModel, Field

    # Define structure
    class TechStack(BaseModel):
        """technology stack recommendation"""
        frontend: str = Field(description="frontend framework")
        backend: str = Field(description="backend framework")
        database: str = Field(description="database system")
        reason: str = Field(description="why this stack works well together")

    parser = JsonOutputParser(pydantic_object=TechStack)

    template = ChatPromptTemplate.from_messages([
        ("system", "You are a technical architect."),
        ("human", "Recommend a tech stack for a {app_type} application"),
        ("human", "Respond with JSON matching this format: {format_instructions}"),
    ])

    chain = template | llm | parser

    result = chain.invoke({
        "app_type": "real-time chat",
        "format_instructions": parser.get_format_instructions(),
    })

    print("\nStructured JSON Output:")
    import json
    print(json.dumps(result, indent=2))

    print_subsection("Why Use Output Parsers?")
    explanation = cleandoc('''
        ‚úÖ Type Safety: Convert strings to structured data
        ‚úÖ Validation: Ensure LLM output matches schema
        ‚úÖ Integration: Easy to use parsed data in code
        ‚úÖ Error Handling: Catch malformed responses
    ''')
    print(f"\n{explanation}")


# endregion


# region 7. Advanced: Partial Variables with Runtime Data


def demo_partial_with_runtime() -> None:
    """
    demonstrate partial variables with runtime-generated data

    Partial Variables: Dynamic Context Injection
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         Partial Variables with Runtime Data                 ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Template Definition:                                       ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
    ‚îÇ  ‚îÇ PromptTemplate(                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   template="{context}\n\nQuestion: {question}",‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   input_variables=["question"],                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   partial_variables={                          ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ     "context": get_current_context  ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ   }                                       ‚îÇ    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ )                                         ‚îÇ    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
    ‚îÇ                                              ‚îÇ              ‚îÇ
    ‚îÇ                                              ‚îÇ              ‚îÇ
    ‚îÇ  Runtime Execution Flow:                     ‚îÇ              ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
    ‚îÇ  ‚îÇ 1. User invokes chain:                    ‚îÇ    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ    chain.invoke({"question": "..."})      ‚îÇ    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                           ‚îÇ    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ 2. Callable executed automatically:       ‚îÇ    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ    def get_current_context() -> str: ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ        return f"Current date: {now()}"         ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ 3. Template formatted:                         ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ    "Current date: 2026-01-18 14:30             ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ     Question: What day is it today?"           ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ                                                ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ 4. LLM processes with fresh context            ‚îÇ         ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Use Cases for Partial Callables:                           ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
    ‚îÇ  ‚îÇ ‚Ä¢ Current date/time (always fresh)             ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ ‚Ä¢ User session data (per-request)              ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ ‚Ä¢ System configuration (runtime values)        ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ ‚Ä¢ Environment context (deployment info)        ‚îÇ         ‚îÇ
    ‚îÇ  ‚îÇ ‚Ä¢ Request metadata (headers, auth)             ‚îÇ         ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: DRY - reuse templates with dynamic data        ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Consistency - same logic across uses           ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Automatic - no manual context injection        ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Type-safe - callable signature validated       ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("7. Partial Variables with Runtime Context")

    print_subsection("Dynamic Context Injection")

    from datetime import datetime

    # Function to get current context
    def get_current_context() -> str:
        """generate runtime context"""
        return f"Current date: {datetime.now().strftime('%Y-%m-%d %H:%M')}"

    # Create template with partial callable
    template = PromptTemplate(
        template=cleandoc('''
            {context}

            You are a helpful assistant.

            Question: {question}

            Provide a brief answer considering the current context:
        '''),
        input_variables=["question"],
        partial_variables={"context": get_current_context},
    )

    # Create chain
    llm = get_llm(temperature=0.7)
    chain = template | llm

    # Use template (context is auto-generated)
    print("\nQuestion: What day is it today?")
    response = chain.invoke({"question": "What day is it today?"})
    print_llm_output("Context-Aware Response", response.content)

    print_subsection("Why Use Partial Callables?")
    explanation = cleandoc('''
        ‚úÖ Dynamic Data: Inject runtime information automatically
        ‚úÖ DRY Principle: Reuse templates with changing context
        ‚úÖ Consistency: Same context logic across all uses

        Use cases:
        ‚Ä¢ Current date/time
        ‚Ä¢ User session data
        ‚Ä¢ System configuration
        ‚Ä¢ Environment context
    ''')
    print(f"\n{explanation}")


# endregion


# region Main


def show_menu(has_openai: bool) -> None:
    """display interactive demo menu"""
    print("\n" + "=" * 70)
    print("  LangChain Prompts & Templates - Practical Examples")
    print("=" * 70)
    print("\nüìö Available Demos:\n")

    demos = [
        ("1", "PromptTemplate with LLM", "template integration with real API calls", True),
        ("2", "ChatPromptTemplate with LLM", "multi-message templates in action", True),
        ("3", "MessagesPlaceholder with Chat History", "conversational context management", True),
        ("4", "FewShotPromptTemplate with LLM", "in-context learning demonstrations", True),
        ("5", "FewShotChatMessagePromptTemplate", "teaching response style", True),
        ("6", "Output Parsers", "structured data extraction", True),
        ("7", "Partial Variables with Runtime", "dynamic context injection", True),
    ]

    for num, name, desc, needs_api in demos:
        api_marker = "üîë" if needs_api else "  "
        status = "" if has_openai else " ‚ö†Ô∏è (needs API key)"
        print(f"  {api_marker} [{num}] {name}")
        print(f"      {desc}{status}")
        print()

    if not has_openai:
        print("  ‚ö†Ô∏è  OpenAI API key required for all demos")
        print("     Set OPENAI_API_KEY in .env file")
        print()

    print("  [a] Run all demos")
    print("  [q] Quit")
    print("\n" + "=" * 70)


def run_selected_demos(selections: str, has_openai: bool) -> bool:
    """run selected demos based on user input"""
    selections = selections.lower().strip()

    if selections == 'q':
        return False

    if not has_openai:
        print("\n‚ö†Ô∏è  Cannot run demos: OPENAI_API_KEY not found")
        print("Please add OPENAI_API_KEY to your .env file")
        print("\nFor conceptual demos without API key, run:")
        print("  uv run python -m phase7_frameworks.01_langchain_basics.01_prompts.concepts")
        return True

    demo_map = {
        '1': ("PromptTemplate with LLM", demo_prompt_template_with_llm),
        '2': ("ChatPromptTemplate with LLM", demo_chat_template_with_llm),
        '3': ("MessagesPlaceholder with Chat History", demo_messages_placeholder_with_llm),
        '4': ("FewShotPromptTemplate with LLM", demo_few_shot_with_llm),
        '5': ("FewShotChatMessagePromptTemplate", demo_few_shot_chat_with_llm),
        '6': ("Output Parsers", demo_output_parsers),
        '7': ("Partial Variables with Runtime", demo_partial_with_runtime),
    }

    if selections == 'a':
        # run all demos
        for name, demo_func in demo_map.values():
            try:
                demo_func()
            except Exception as e:
                print(f"\n‚ùå Error in {name}: {e}")
    else:
        # parse comma-separated selections
        selected = [s.strip() for s in selections.split(',')]
        for sel in selected:
            if sel in demo_map:
                name, demo_func = demo_map[sel]
                try:
                    demo_func()
                except Exception as e:
                    print(f"\n‚ùå Error in {name}: {e}")
            else:
                print(f"‚ö†Ô∏è  Invalid selection: {sel}")

    return True


def main() -> None:
    """run demonstrations with interactive menu"""
    has_openai, _ = check_api_keys()

    print("\n" + "=" * 70)
    print("  LangChain Prompts & Templates - Practical Examples")
    print("  Using OpenAI API for real LLM integration")
    print("=" * 70)

    while True:
        show_menu(has_openai)
        selection = input("\nSelect demos to run (comma-separated) or 'a' for all: ").strip()

        if not selection:
            continue

        if not run_selected_demos(selection, has_openai):
            break

        print("\n" + "=" * 70)
        print("  Demos complete!")
        print("=" * 70)

        # pause before showing menu again
        try:
            input("\n‚è∏Ô∏è  Press Enter to continue...")
        except (EOFError, KeyboardInterrupt):
            print("\n\nüëã Goodbye!")
            break

    print("\n" + "=" * 70)
    print("  Thanks for exploring LangChain prompts!")
    print("  You now understand prompt templates with real LLM integration")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüëã Goodbye!")


# endregion
