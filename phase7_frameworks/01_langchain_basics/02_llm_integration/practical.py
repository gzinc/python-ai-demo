"""
LLM Integration - Hands-On Practice (Requires API Keys)

Demonstrates real LLM integration with OpenAI and Anthropic using LangChain's
unified interface. Requires OPENAI_API_KEY and/or ANTHROPIC_API_KEY in .env

Run: uv run python -m phase7_frameworks.01_langchain_basics.02_llm_integration.practical
"""

from inspect import cleandoc

from langchain_core.messages import HumanMessage, SystemMessage

from phase7_frameworks.utils import (
    check_api_keys,
    print_section,
    requires_anthropic,
    requires_both_keys,
    requires_openai,
)


# region Demo 1: ChatOpenAI Basic Usage


def demo_chatopenai_basic() -> None:
    """
    demonstrate basic ChatOpenAI usage

    ChatOpenAI Integration Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ChatOpenAI: Unified OpenAI API Integration          ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  1. Model Initialization:                                   ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ ChatOpenAI(                          ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ   model="gpt-3.5-turbo",             ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ   temperature=0.7,                   ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ   max_tokens=100                     ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ )                                    ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  2. Single Message Flow:                                    ‚îÇ
    ‚îÇ     [HumanMessage("What are embeddings?")]                  ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ     llm.invoke(messages)                                    ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ     AIMessage(content="Embeddings are...")                  ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  3. Multi-Turn Conversation:                                ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
    ‚îÇ     ‚îÇ [SystemMessage("You are..."),      ‚îÇ                  ‚îÇ
    ‚îÇ     ‚îÇ  HumanMessage("How do I...?")]     ‚îÇ                  ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ     llm.invoke(conversation)                                ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ     AIMessage with contextual response                      ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Unified interface across OpenAI models         ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Simple configuration (model, temp, tokens)     ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Automatic message formatting                   ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Built-in retry logic and error handling        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("Demo 1: ChatOpenAI Basic Usage")

    has_openai, _ = check_api_keys()
    if not has_openai:
        print("‚ö†Ô∏è  OPENAI_API_KEY not found - skipping demo")
        print("Set OPENAI_API_KEY in .env to run this demo")
        return

    from langchain_openai import ChatOpenAI

    # Initialize chat model
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",  # using cheaper model for demo
        temperature=0.7,
        max_tokens=100,
    )

    print("## Model Configuration:")
    print(f"Model: {llm.model_name}")
    print(f"Temperature: {llm.temperature}")
    print(f"Max Tokens: {llm.max_tokens}")

    # Single message
    print("\n## Single Message:")
    messages = [HumanMessage(content="What are embeddings in one sentence?")]
    response = llm.invoke(messages)
    print(f"Response: {response.content}")

    # Multi-turn conversation
    print("\n## Multi-Turn Conversation:")
    conversation = [
        SystemMessage(content="You are a concise Python expert"),
        HumanMessage(content="How do I read a file?"),
    ]
    response = llm.invoke(conversation)
    print(f"Response: {response.content[:200]}...")


# endregion

# region Demo 2: ChatAnthropic Basic Usage


def demo_chatanthropic_basic() -> None:
    """
    demonstrate basic ChatAnthropic usage

    ChatAnthropic Integration Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       ChatAnthropic: Unified Anthropic API Integration      ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  1. Model Initialization:                                   ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ ChatAnthropic(                       ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ   model="claude-3-5-haiku-20241022", ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ   temperature=0.7,                   ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ   max_tokens=100                     ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ )                                    ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ  2. Message Flow (Same as ChatOpenAI):                      ‚îÇ
    ‚îÇ     [HumanMessage("What are embeddings?")]                  ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ     llm.invoke(messages)                                    ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ                    ‚ñº                                        ‚îÇ
    ‚îÇ     AIMessage(content="Embeddings are...")                  ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Provider Interchangeability:                               ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
    ‚îÇ  ‚îÇ  ChatOpenAI    ‚îÇ         ‚îÇ ChatAnthropic  ‚îÇ              ‚îÇ
    ‚îÇ  ‚îÇ  (GPT models)  ‚îÇ   ‚Üê‚Üí    ‚îÇ (Claude models)‚îÇ              ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
    ‚îÇ         ‚Üì                            ‚Üì                      ‚îÇ
    ‚îÇ    Same .invoke() interface                                 ‚îÇ
    ‚îÇ    Same message types                                       ‚îÇ
    ‚îÇ    Same response format                                     ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Identical interface to ChatOpenAI              ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Seamless provider switching                    ‚îÇ
    ‚îÇ  ‚úÖ Benefit: No code changes needed for migration           ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Multi-provider fallback support                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("Demo 2: ChatAnthropic Basic Usage")

    _, has_anthropic = check_api_keys()
    if not has_anthropic:
        print("‚ö†Ô∏è  ANTHROPIC_API_KEY not found - skipping demo")
        print("Set ANTHROPIC_API_KEY in .env to run this demo")
        return

    from langchain_anthropic import ChatAnthropic

    # Initialize chat model
    llm = ChatAnthropic(
        model="claude-3-5-haiku-20241022",  # using cheaper model for demo
        temperature=0.7,
        max_tokens=100,
    )

    print("## Model Configuration:")
    print(f"Model: {llm.model}")
    print(f"Temperature: {llm.temperature}")
    print(f"Max Tokens: {llm.max_tokens}")

    # Single message
    print("\n## Single Message:")
    messages = [HumanMessage(content="What are embeddings in one sentence?")]
    response = llm.invoke(messages)
    print(f"Response: {response.content}")

    # Multi-turn conversation
    print("\n## Multi-Turn Conversation:")
    conversation = [
        SystemMessage(content="You are a concise Python expert"),
        HumanMessage(content="How do I read a file?"),
    ]
    response = llm.invoke(conversation)
    print(f"Response: {response.content[:200]}...")


# endregion

# region Demo 3: Temperature and Creativity Control


@requires_openai
def demo_temperature_control() -> None:
    """
    demonstrate how temperature affects output

    Temperature Control Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       Temperature: Controlling Output Randomness            ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Same Prompt, Different Temperatures:                       ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Temperature 0.0 (Deterministic):                           ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ "AI-powered headphones use advanced  ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  noise cancellation. They adapt to   ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  your listening environment."        ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ     ‚Ä¢ Most likely tokens chosen every time                  ‚îÇ
    ‚îÇ     ‚Ä¢ Consistent, repeatable responses                      ‚îÇ
    ‚îÇ     ‚Ä¢ Best for: factual tasks, data extraction              ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Temperature 0.5 (Balanced):                                ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ "These AI headphones deliver smart   ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  noise cancellation with adaptive    ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  audio optimization."                ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ     ‚Ä¢ Moderate randomness in token selection                ‚îÇ
    ‚îÇ     ‚Ä¢ More variety, still coherent                          ‚îÇ
    ‚îÇ     ‚Ä¢ Best for: general chat, recommendations               ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Temperature 1.0 (Creative):                                ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ "Experience revolutionary AI audio   ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  that learns your preferences and    ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ  transforms your listening journey." ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ     ‚Ä¢ High randomness in token selection                    ‚îÇ
    ‚îÇ     ‚Ä¢ Most creative and varied                              ‚îÇ
    ‚îÇ     ‚Ä¢ Best for: creative writing, brainstorming             ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Temperature Scale:                                         ‚îÇ
    ‚îÇ  0.0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0.5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 1.0 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 2.0           ‚îÇ
    ‚îÇ  Deterministic   Balanced       Creative      Chaotic       ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Fine-tune creativity vs consistency            ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Task-specific optimization                     ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Control output variability                     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("Demo 3: Temperature and Creativity Control")


    from langchain_openai import ChatOpenAI

    prompt = "Write a two-sentence product description for AI-powered headphones"
    messages = [HumanMessage(content=prompt)]

    print(f"Prompt: {prompt}\n")

    temperatures = [0.0, 0.5, 1.0]
    for temp in temperatures:
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=temp, max_tokens=100)
        response = llm.invoke(messages)

        print(f"Temperature {temp}:")
        print(f"‚Üí {response.content}\n")

    print("Notice how higher temperature produces more varied/creative responses")


# endregion

# region Demo 4: Streaming Responses


@requires_openai
def demo_streaming() -> None:
    """
    demonstrate real-time streaming responses

    Streaming Response Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         Streaming: Real-Time Token-by-Token Delivery        ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Non-Streaming (llm.invoke()):                              ‚îÇ
    ‚îÇ     User sends request                                      ‚îÇ
    ‚îÇ            ‚îÇ                                                ‚îÇ
    ‚îÇ            ‚ñº                                                ‚îÇ
    ‚îÇ     ‚è≥ Wait for full response...                            ‚îÇ
    ‚îÇ            ‚îÇ                                                ‚îÇ
    ‚îÇ            ‚ñº                                                ‚îÇ
    ‚îÇ     Complete response delivered at once                     ‚îÇ
    ‚îÇ     "RAG is Retrieval-Augmented Generation..."              ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Streaming (llm.stream()):                                  ‚îÇ
    ‚îÇ     User sends request                                      ‚îÇ
    ‚îÇ            ‚îÇ                                                ‚îÇ
    ‚îÇ            ‚ñº                                                ‚îÇ
    ‚îÇ     Token 1: "RAG"          ‚Üê Immediate                     ‚îÇ
    ‚îÇ     Token 2: " is"          ‚Üê 50ms                          ‚îÇ
    ‚îÇ     Token 3: " Retrieval"   ‚Üê 100ms                         ‚îÇ
    ‚îÇ     Token 4: "-Augmented"   ‚Üê 150ms                         ‚îÇ
    ‚îÇ     Token 5: " Generation"  ‚Üê 200ms                         ‚îÇ
    ‚îÇ     ...                                                     ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Implementation:                                            ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ for chunk in llm.stream(messages):   ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ     print(chunk.content, end="")     ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  User Experience Comparison:                                ‚îÇ
    ‚îÇ  Non-Streaming:  ‚è≥‚è≥‚è≥‚è≥ ‚Üí Full response                  ‚îÇ
    ‚îÇ  Streaming:      R ‚Üí RA ‚Üí RAG ‚Üí RAG is ‚Üí ...                ‚îÇ
    ‚îÇ                  ‚Üë Feels faster, more interactive           ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Better perceived performance                   ‚îÇ
    ‚îÇ  ‚úÖ Benefit: User sees progress immediately                 ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Can stop generation early                      ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Improved UX for long responses                 ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("Demo 4: Streaming Responses")


    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, streaming=True)

    messages = [
        SystemMessage(content="You are a helpful assistant"),
        HumanMessage(content="Explain what RAG is in 2-3 sentences"),
    ]

    print("## Streaming Response (tokens arrive in real-time):\n")
    for chunk in llm.stream(messages):
        print(chunk.content, end="", flush=True)
    print("\n")

    print("‚úì Streaming provides better UX for interactive applications")


# endregion

# region Demo 5: Provider Switching


def demo_provider_switching() -> None:
    """
    demonstrate switching between providers seamlessly

    Provider Switching Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ      Seamless Provider Switching: Same Interface            ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Unified Message Interface:                                 ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                  ‚îÇ
    ‚îÇ     ‚îÇ messages = [                       ‚îÇ                  ‚îÇ
    ‚îÇ     ‚îÇ   SystemMessage("You are..."),     ‚îÇ                  ‚îÇ
    ‚îÇ     ‚îÇ   HumanMessage("What are...?")     ‚îÇ                  ‚îÇ
    ‚îÇ     ‚îÇ ]                                  ‚îÇ                  ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îÇ
    ‚îÇ                    ‚îÇ                                        ‚îÇ
    ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
    ‚îÇ         ‚îÇ                     ‚îÇ                             ‚îÇ
    ‚îÇ         ‚ñº                     ‚ñº                             ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
    ‚îÇ  ‚îÇ  OpenAI    ‚îÇ        ‚îÇ Anthropic  ‚îÇ                       ‚îÇ
    ‚îÇ  ‚îÇ  GPT-3.5   ‚îÇ        ‚îÇ Claude 3.5 ‚îÇ                       ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
    ‚îÇ        ‚îÇ                     ‚îÇ                              ‚îÇ
    ‚îÇ        ‚ñº                     ‚ñº                              ‚îÇ
    ‚îÇ  openai_llm.invoke()   anthropic_llm.invoke()               ‚îÇ
    ‚îÇ        ‚îÇ                     ‚îÇ                              ‚îÇ
    ‚îÇ        ‚ñº                     ‚ñº                              ‚îÇ
    ‚îÇ  AIMessage(...)         AIMessage(...)                      ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Provider Comparison:                                       ‚îÇ
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
    ‚îÇ  ‚îÇ Feature     ‚îÇ OpenAI       ‚îÇ Anthropic    ‚îÇ              ‚îÇ
    ‚îÇ  ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§              ‚îÇ
    ‚îÇ  ‚îÇ Interface   ‚îÇ ChatOpenAI   ‚îÇ ChatAnthropic‚îÇ              ‚îÇ
    ‚îÇ  ‚îÇ invoke()    ‚îÇ ‚úì Same       ‚îÇ ‚úì Same       ‚îÇ              ‚îÇ
    ‚îÇ  ‚îÇ stream()    ‚îÇ ‚úì Same       ‚îÇ ‚úì Same       ‚îÇ              ‚îÇ
    ‚îÇ  ‚îÇ Messages    ‚îÇ ‚úì Same types ‚îÇ ‚úì Same types ‚îÇ              ‚îÇ
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Migration Example:                                         ‚îÇ
    ‚îÇ     # Before: OpenAI                                        ‚îÇ
    ‚îÇ     llm = ChatOpenAI(model="gpt-3.5-turbo")                 ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ     # After: Anthropic (only 1 line changes!)               ‚îÇ
    ‚îÇ     llm = ChatAnthropic(model="claude-3-5-haiku-20241022")  ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Zero code changes for provider switch          ‚îÇ
    ‚îÇ  ‚úÖ Benefit: A/B test different providers easily            ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Multi-provider applications                    ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Vendor independence                            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("Demo 5: Provider Switching")

    has_openai, has_anthropic = check_api_keys()

    if not (has_openai and has_anthropic):
        print("‚ö†Ô∏è  Both API keys needed for this demo")
        print("Set OPENAI_API_KEY and ANTHROPIC_API_KEY in .env")
        return

    from langchain_anthropic import ChatAnthropic
    from langchain_openai import ChatOpenAI

    # Same prompt for both providers
    messages = [
        SystemMessage(content="You are a concise AI expert"),
        HumanMessage(content="What are embeddings in one sentence?"),
    ]

    # OpenAI
    print("## OpenAI GPT-3.5:")
    openai_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    openai_response = openai_llm.invoke(messages)
    print(f"‚Üí {openai_response.content}")

    # Anthropic
    print("\n## Anthropic Claude:")
    anthropic_llm = ChatAnthropic(
        model="claude-3-5-haiku-20241022", temperature=0.7, max_tokens=100
    )
    anthropic_response = anthropic_llm.invoke(messages)
    print(f"‚Üí {anthropic_response.content}")

    print("\n‚úì Same interface, different providers - seamless switching!")


# endregion

# region Demo 6: Fallback Chains


def demo_fallback_chain() -> None:
    """
    demonstrate fallback from primary to secondary provider

    Fallback Chain Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ        Fallback Chains: Resilient Multi-Provider Setup      ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Request Flow with Fallback:                                ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ     User Request                                            ‚îÇ
    ‚îÇ          ‚îÇ                                                  ‚îÇ
    ‚îÇ          ‚ñº                                                  ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                     ‚îÇ
    ‚îÇ     ‚îÇ Primary: GPT-4  ‚îÇ ‚Üê Try first (high quality)          ‚îÇ
    ‚îÇ     ‚îÇ timeout=5s      ‚îÇ                                     ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                     ‚îÇ
    ‚îÇ              ‚îÇ                                              ‚îÇ
    ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                    ‚îÇ
    ‚îÇ    ‚îÇ                   ‚îÇ                                    ‚îÇ
    ‚îÇ    ‚ñº                   ‚ñº                                    ‚îÇ
    ‚îÇ Success?           Timeout/Error?                           ‚îÇ
    ‚îÇ    ‚îÇ                   ‚îÇ                                    ‚îÇ
    ‚îÇ    ‚îÇ                   ‚ñº                                    ‚îÇ
    ‚îÇ    ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                          ‚îÇ
    ‚îÇ    ‚îÇ          ‚îÇ Fallback: Claude ‚îÇ ‚Üê Different provider     ‚îÇ
    ‚îÇ    ‚îÇ          ‚îÇ 3.5 Haiku        ‚îÇ                          ‚îÇ
    ‚îÇ    ‚îÇ          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                          ‚îÇ
    ‚îÇ    ‚îÇ                   ‚îÇ                                    ‚îÇ
    ‚îÇ    ‚îÇ          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                           ‚îÇ
    ‚îÇ    ‚îÇ          ‚îÇ                 ‚îÇ                           ‚îÇ
    ‚îÇ    ‚îÇ          ‚ñº                 ‚ñº                           ‚îÇ
    ‚îÇ    ‚îÇ       Success?         Error?                          ‚îÇ
    ‚îÇ    ‚îÇ          ‚îÇ                 ‚îÇ                           ‚îÇ
    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚Üí Return response/error   ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Implementation:                                            ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ primary = ChatOpenAI(...)            ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ fallback = ChatAnthropic(...)        ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ                                      ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ llm = primary.with_fallbacks(        ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ     [fallback]                       ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ )                                    ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Fallback Scenarios:                                        ‚îÇ
    ‚îÇ  ‚Ä¢ Primary timeout ‚Üí Try fallback                           ‚îÇ
    ‚îÇ  ‚Ä¢ API rate limit ‚Üí Switch provider                         ‚îÇ
    ‚îÇ  ‚Ä¢ Model unavailable ‚Üí Use alternative                      ‚îÇ
    ‚îÇ  ‚Ä¢ Cost optimization ‚Üí Cheap primary, quality fallback      ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Increased reliability (99.9%+ uptime)          ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Vendor independence and resilience             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Cost optimization strategies                   ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Graceful degradation                           ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("Demo 6: Fallback Chains")

    has_openai, has_anthropic = check_api_keys()

    if not (has_openai and has_anthropic):
        print("‚ö†Ô∏è  Both API keys needed for this demo")
        print("Set OPENAI_API_KEY and ANTHROPIC_API_KEY in .env")
        return

    from langchain_anthropic import ChatAnthropic
    from langchain_openai import ChatOpenAI

    # Primary: GPT-4 (expensive, high quality)
    primary = ChatOpenAI(model="gpt-4", temperature=0.7, max_tokens=100, timeout=5)

    # Fallback: Claude (different provider)
    fallback = ChatAnthropic(
        model="claude-3-5-haiku-20241022", temperature=0.7, max_tokens=100
    )

    # Chain with fallback
    llm_with_fallback = primary.with_fallbacks([fallback])

    print("## Fallback Chain: GPT-4 ‚Üí Claude")
    print("Primary: GPT-4 (if it fails...)")
    print("Fallback: Claude-3.5-Haiku\n")

    messages = [HumanMessage(content="What are embeddings?")]

    try:
        response = llm_with_fallback.invoke(messages)
        print(f"Response: {response.content[:200]}...")
        print("\n‚úì Request succeeded (primary or fallback)")
    except Exception as e:
        print(f"‚ùå Both providers failed: {e}")


# endregion

# region Demo 7: Token Usage Tracking


@requires_openai
def demo_token_tracking() -> None:
    """
    demonstrate tracking token usage and costs

    Token Usage Tracking Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       Token Tracking: Cost Monitoring and Optimization      ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Request with Callback Tracking:                            ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ     with get_openai_callback() as cb:                       ‚îÇ
    ‚îÇ          ‚îÇ                                                  ‚îÇ
    ‚îÇ          ‚ñº                                                  ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
    ‚îÇ     ‚îÇ User Prompt (Input)         ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îÇ "Explain embeddings..."     ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
    ‚îÇ                ‚îÇ                                            ‚îÇ
    ‚îÇ                ‚ñº                                            ‚îÇ
    ‚îÇ     Tokenization: 15 tokens                                 ‚îÇ
    ‚îÇ                ‚îÇ                                            ‚îÇ
    ‚îÇ                ‚ñº                                            ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
    ‚îÇ     ‚îÇ LLM Processing              ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îÇ (GPT-3.5-turbo)             ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
    ‚îÇ                ‚îÇ                                            ‚îÇ
    ‚îÇ                ‚ñº                                            ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
    ‚îÇ     ‚îÇ AI Response (Output)          ‚îÇ                       ‚îÇ
    ‚îÇ     ‚îÇ "Embeddings are numerical..." ‚îÇ                       ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
    ‚îÇ                ‚îÇ                                            ‚îÇ
    ‚îÇ                ‚ñº                                            ‚îÇ
    ‚îÇ     Tokenization: 85 tokens                                 ‚îÇ
    ‚îÇ                ‚îÇ                                            ‚îÇ
    ‚îÇ                ‚ñº                                            ‚îÇ
    ‚îÇ     Callback Records:                                       ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ
    ‚îÇ     ‚îÇ Prompt tokens:     15             ‚îÇ                   ‚îÇ
    ‚îÇ     ‚îÇ Completion tokens: 85             ‚îÇ                   ‚îÇ
    ‚îÇ     ‚îÇ Total tokens:      100            ‚îÇ                   ‚îÇ
    ‚îÇ     ‚îÇ                                   ‚îÇ                   ‚îÇ
    ‚îÇ     ‚îÇ Cost Calculation:                 ‚îÇ                   ‚îÇ
    ‚îÇ     ‚îÇ Input:  15 √ó $0.50/1M = $0.000008 ‚îÇ                   ‚îÇ
    ‚îÇ     ‚îÇ Output: 85 √ó $1.50/1M = $0.000128 ‚îÇ                   ‚îÇ
    ‚îÇ     ‚îÇ Total:                  $0.000136 ‚îÇ                   ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Cost Management:                                           ‚îÇ
    ‚îÇ  ‚Ä¢ Track per-request costs                                  ‚îÇ
    ‚îÇ  ‚Ä¢ Set budget alerts ($X per day/month)                     ‚îÇ
    ‚îÇ  ‚Ä¢ Compare provider costs (GPT-3.5 vs GPT-4 vs Claude)      ‚îÇ
    ‚îÇ  ‚Ä¢ Optimize prompt length (fewer input tokens)              ‚îÇ
    ‚îÇ  ‚Ä¢ Cache common responses                                   ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Real-time cost monitoring                      ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Budget control and alerts                      ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Provider cost comparison                       ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Optimization insights                          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("Demo 7: Token Usage Tracking")


    from langchain.callbacks import get_openai_callback
    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    messages = [
        SystemMessage(content="You are a helpful assistant"),
        HumanMessage(content="Explain embeddings in simple terms"),
    ]

    print("## Tracking Token Usage with Callbacks:\n")

    with get_openai_callback() as cb:
        response = llm.invoke(messages)

        print(f"Response: {response.content[:100]}...\n")

        print("Token Usage:")
        print(f"  Prompt tokens: {cb.prompt_tokens}")
        print(f"  Completion tokens: {cb.completion_tokens}")
        print(f"  Total tokens: {cb.total_tokens}")
        print(f"  Total cost: ${cb.total_cost:.6f}")

        # Calculate cost breakdown
        print("\nCost Breakdown (GPT-3.5-turbo pricing):")
        print(f"  Input cost:  ${cb.prompt_tokens / 1_000_000 * 0.50:.6f}")
        print(f"  Output cost: ${cb.completion_tokens / 1_000_000 * 1.50:.6f}")


# endregion

# region Demo 8: LCEL Integration


@requires_openai
def demo_lcel_integration() -> None:
    """
    demonstrate LangChain Expression Language integration

    LCEL Integration Pattern:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ       LCEL: Composable LLM Chains with Pipe Operator        ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Chain Composition: Prompt | LLM | Parser                   ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ     Input: {"concept": "embeddings"}                        ‚îÇ
    ‚îÇ          ‚îÇ                                                  ‚îÇ
    ‚îÇ          ‚ñº                                                  ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
    ‚îÇ     ‚îÇ ChatPromptTemplate          ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îÇ "Explain {concept} in       ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îÇ  one sentence"              ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
    ‚îÇ                ‚îÇ                                            ‚îÇ
    ‚îÇ                ‚îÇ Formatted prompt                           ‚îÇ
    ‚îÇ                ‚ñº                                            ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
    ‚îÇ     ‚îÇ ChatOpenAI                  ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îÇ (gpt-3.5-turbo)             ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
    ‚îÇ                ‚îÇ                                            ‚îÇ
    ‚îÇ                ‚îÇ AIMessage                                  ‚îÇ
    ‚îÇ                ‚ñº                                            ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
    ‚îÇ     ‚îÇ StrOutputParser             ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îÇ Extract .content            ‚îÇ                         ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
    ‚îÇ                ‚îÇ                                            ‚îÇ
    ‚îÇ                ‚ñº                                            ‚îÇ
    ‚îÇ     Output: "Embeddings are numerical..."                   ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  LCEL Syntax:                                               ‚îÇ
    ‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
    ‚îÇ     ‚îÇ chain = (                            ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ     template                         ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ     | llm                            ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ     | parser                         ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ )                                    ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ                                      ‚îÇ                ‚îÇ
    ‚îÇ     ‚îÇ result = chain.invoke(input)         ‚îÇ                ‚îÇ
    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  Multiple Concepts in Batch:                                ‚îÇ
    ‚îÇ     concepts = ["embeddings", "RAG", "fine-tuning"]         ‚îÇ
    ‚îÇ          ‚îÇ                                                  ‚îÇ
    ‚îÇ          ‚ñº                                                  ‚îÇ
    ‚îÇ     Same chain processes all three                          ‚îÇ
    ‚îÇ          ‚îÇ                                                  ‚îÇ
    ‚îÇ          ‚ñº                                                  ‚îÇ
    ‚îÇ     Three concise explanations                              ‚îÇ
    ‚îÇ                                                             ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Clean, readable composition                    ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Reusable chain components                      ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Type-safe data flow                            ‚îÇ
    ‚îÇ  ‚úÖ Benefit: Easy to extend and modify                      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    """
    print_section("Demo 8: LCEL Integration with LLMs")


    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    # Create a chain: prompt | llm | parser
    template = ChatPromptTemplate.from_messages(
        [
            ("system", "You are a concise AI expert"),
            ("human", "Explain {concept} in one sentence"),
        ]
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    parser = StrOutputParser()

    # LCEL chain composition
    chain = template | llm | parser

    print("## LCEL Chain: Prompt | LLM | Parser\n")

    concepts = ["embeddings", "RAG", "fine-tuning"]
    for concept in concepts:
        result = chain.invoke({"concept": concept})
        print(f"{concept:15s}: {result}")

    print("\n‚úì LCEL makes it easy to compose LLMs with prompts and parsers")


# endregion


def show_menu(has_openai: bool, has_anthropic: bool) -> None:
    """display interactive demo menu"""
    print("\n" + "=" * 70)
    print("  LLM Integration - Practical Examples")
    print("=" * 70)
    print("\nüìö Available Demos:\n")

    demos = [
        ("1", "ChatOpenAI Basic Usage", "simple OpenAI integration", 'openai'),
        ("2", "ChatAnthropic Basic Usage", "simple Anthropic integration", 'anthropic'),
        ("3", "Temperature and Creativity Control", "output randomness tuning", 'openai'),
        ("4", "Streaming Responses", "real-time token delivery", 'openai'),
        ("5", "Provider Switching", "seamless provider swapping", 'both'),
        ("6", "Fallback Chains", "multi-provider reliability", 'both'),
        ("7", "Token Usage Tracking", "cost monitoring and optimization", 'openai'),
        ("8", "LCEL Integration", "chain composition with LLMs", 'openai'),
    ]

    for num, name, desc, requires in demos:
        needs_api = requires in ('openai', 'both')
        needs_anthropic = requires in ('anthropic', 'both')

        has_required = (
            (not needs_api or has_openai) and
            (not needs_anthropic or has_anthropic)
        )

        api_marker = "üîë" if (needs_api or needs_anthropic) else "  "

        if not has_required:
            if requires == 'both':
                status = " ‚ö†Ô∏è (needs both API keys)"
            elif requires == 'openai':
                status = " ‚ö†Ô∏è (needs OpenAI key)"
            else:
                status = " ‚ö†Ô∏è (needs Anthropic key)"
        else:
            status = ""

        print(f"  {api_marker} [{num}] {name}")
        print(f"      {desc}{status}")
        print()

    if not (has_openai or has_anthropic):
        print("  ‚ö†Ô∏è  At least one API key required")
        print("     Set OPENAI_API_KEY and/or ANTHROPIC_API_KEY in .env")
        print()

    print("  [a] Run all demos")
    print("  [q] Quit")
    print("\n" + "=" * 70)


def run_selected_demos(selections: str, has_openai: bool, has_anthropic: bool) -> bool:
    """run selected demos based on user input"""
    selections = selections.lower().strip()

    if selections == 'q':
        return False

    demo_map = {
        '1': ("ChatOpenAI Basic", demo_chatopenai_basic, 'openai'),
        '2': ("ChatAnthropic Basic", demo_chatanthropic_basic, 'anthropic'),
        '3': ("Temperature Control", demo_temperature_control, 'openai'),
        '4': ("Streaming", demo_streaming, 'openai'),
        '5': ("Provider Switching", demo_provider_switching, 'both'),
        '6': ("Fallback Chain", demo_fallback_chain, 'both'),
        '7': ("Token Tracking", demo_token_tracking, 'openai'),
        '8': ("LCEL Integration", demo_lcel_integration, 'openai'),
    }

    if selections == 'a':
        # run all demos
        for name, demo_func, requires in demo_map.values():
            if requires == 'openai' and not has_openai:
                print(f"\n‚ö†Ô∏è  Skipping {name}: OpenAI API key required")
                continue
            if requires == 'anthropic' and not has_anthropic:
                print(f"\n‚ö†Ô∏è  Skipping {name}: Anthropic API key required")
                continue
            if requires == 'both' and not (has_openai and has_anthropic):
                print(f"\n‚ö†Ô∏è  Skipping {name}: Both API keys required")
                continue
            try:
                demo_func()
            except Exception as e:
                print(f"\n‚ùå Error in {name}: {e}")
    else:
        # parse comma-separated selections
        selected = [s.strip() for s in selections.split(',')]
        for sel in selected:
            if sel in demo_map:
                name, demo_func, requires = demo_map[sel]

                # check API key requirements
                if requires == 'openai' and not has_openai:
                    print(f"\n‚ö†Ô∏è  Cannot run {name}: OpenAI API key required")
                    continue
                if requires == 'anthropic' and not has_anthropic:
                    print(f"\n‚ö†Ô∏è  Cannot run {name}: Anthropic API key required")
                    continue
                if requires == 'both' and not (has_openai and has_anthropic):
                    print(f"\n‚ö†Ô∏è  Cannot run {name}: Both API keys required")
                    continue

                try:
                    demo_func()
                except Exception as e:
                    print(f"\n‚ùå Error in {name}: {e}")
            else:
                print(f"‚ö†Ô∏è  Invalid selection: {sel}")

    return True


def main() -> None:
    """run demonstrations with interactive menu"""
    has_openai, has_anthropic = check_api_keys()

    print("\n" + "=" * 70)
    print("  LLM Integration - Practical Examples")
    print("  Real API calls to OpenAI and Anthropic")
    print("=" * 70)
    print("\n## API Key Status:")
    print(f"OPENAI_API_KEY: {'‚úì Found' if has_openai else '‚úó Missing'}")
    print(f"ANTHROPIC_API_KEY: {'‚úì Found' if has_anthropic else '‚úó Missing'}")

    if not (has_openai or has_anthropic):
        print("\n‚ùå No API keys found!")
        print("Set at least one API key in .env to run demos:")
        print("  OPENAI_API_KEY=your-key-here")
        print("  ANTHROPIC_API_KEY=your-key-here")
        return

    while True:
        show_menu(has_openai, has_anthropic)
        selection = input("\nSelect demos to run (comma-separated) or 'a' for all: ").strip()

        if not selection:
            continue

        if not run_selected_demos(selection, has_openai, has_anthropic):
            break

        print("\n" + "=" * 70)
        print("  Demos complete!")
        print("=" * 70)

        # pause before showing menu again
        try:
            input("\n‚è∏Ô∏è  Press Enter to continue...")
        except (EOFError, KeyboardInterrupt):
            print("\n\nüëã Goodbye!")
            break

    print("\n" + "=" * 70)
    print("  Thanks for exploring LLM integration!")
    print("  You've mastered real-world LLM API usage")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüëã Goodbye!")
