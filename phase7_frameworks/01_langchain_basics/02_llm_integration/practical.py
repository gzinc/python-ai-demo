"""
LLM Integration - Hands-On Practice (Requires API Keys)

Demonstrates real LLM integration with OpenAI and Anthropic using LangChain's
unified interface. Requires OPENAI_API_KEY and/or ANTHROPIC_API_KEY in .env

Run: uv run python -m phase7_frameworks.01_langchain_basics.02_llm_integration.practical
"""

from inspect import cleandoc

from langchain_core.messages import HumanMessage, SystemMessage

from common.util.utils import (
from common.demo_menu import Demo, MenuRunner
    check_api_keys,
    print_section,
    requires_anthropic,
    requires_both_keys,
    requires_openai,
)


# region Demo 1: ChatOpenAI Basic Usage


def demo_chatopenai_basic() -> None:
    """
    demonstrate basic ChatOpenAI usage

    ChatOpenAI Integration Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         ChatOpenAI: Unified OpenAI API Integration          â”‚
    â”‚                                                             â”‚
    â”‚  1. Model Initialization:                                   â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚     â”‚ ChatOpenAI(                          â”‚                â”‚
    â”‚     â”‚   model="gpt-3.5-turbo",             â”‚                â”‚
    â”‚     â”‚   temperature=0.7,                   â”‚                â”‚
    â”‚     â”‚   max_tokens=100                     â”‚                â”‚
    â”‚     â”‚ )                                    â”‚                â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚                    â”‚                                        â”‚
    â”‚                    â–¼                                        â”‚
    â”‚  2. Single Message Flow:                                    â”‚
    â”‚     [HumanMessage("What are embeddings?")]                  â”‚
    â”‚                    â”‚                                        â”‚
    â”‚                    â–¼                                        â”‚
    â”‚     llm.invoke(messages)                                    â”‚
    â”‚                    â”‚                                        â”‚
    â”‚                    â–¼                                        â”‚
    â”‚     AIMessage(content="Embeddings are...")                  â”‚
    â”‚                                                             â”‚
    â”‚  3. Multi-Turn Conversation:                                â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
    â”‚     â”‚ [SystemMessage("You are..."),      â”‚                  â”‚
    â”‚     â”‚  HumanMessage("How do I...?")]     â”‚                  â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
    â”‚                    â”‚                                        â”‚
    â”‚                    â–¼                                        â”‚
    â”‚     llm.invoke(conversation)                                â”‚
    â”‚                    â”‚                                        â”‚
    â”‚                    â–¼                                        â”‚
    â”‚     AIMessage with contextual response                      â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Unified interface across OpenAI models         â”‚
    â”‚  âœ… Benefit: Simple configuration (model, temp, tokens)     â”‚
    â”‚  âœ… Benefit: Automatic message formatting                   â”‚
    â”‚  âœ… Benefit: Built-in retry logic and error handling        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 1: ChatOpenAI Basic Usage")

    has_openai, _ = check_api_keys()
    if not has_openai:
        print("âš ï¸  OPENAI_API_KEY not found - skipping demo")
        print("Set OPENAI_API_KEY in .env to run this demo")
        return

    from langchain_openai import ChatOpenAI

    # Initialize chat model
    llm = ChatOpenAI(
        model="gpt-3.5-turbo",  # using cheaper model for demo
        temperature=0.7,
        max_tokens=100,
    )

    print("## Model Configuration:")
    print(f"Model: {llm.model_name}")
    print(f"Temperature: {llm.temperature}")
    print(f"Max Tokens: {llm.max_tokens}")

    # Single message
    print("\n## Single Message:")
    messages = [HumanMessage(content="What are embeddings in one sentence?")]
    response = llm.invoke(messages)
    print(f"Response: {response.content}")

    # Multi-turn conversation
    print("\n## Multi-Turn Conversation:")
    conversation = [
        SystemMessage(content="You are a concise Python expert"),
        HumanMessage(content="How do I read a file?"),
    ]
    response = llm.invoke(conversation)
    print(f"Response: {response.content[:200]}...")


# endregion

# region Demo 2: ChatAnthropic Basic Usage


def demo_chatanthropic_basic() -> None:
    """
    demonstrate basic ChatAnthropic usage

    ChatAnthropic Integration Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       ChatAnthropic: Unified Anthropic API Integration      â”‚
    â”‚                                                             â”‚
    â”‚  1. Model Initialization:                                   â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚     â”‚ ChatAnthropic(                       â”‚                â”‚
    â”‚     â”‚   model="claude-3-5-haiku-20241022", â”‚                â”‚
    â”‚     â”‚   temperature=0.7,                   â”‚                â”‚
    â”‚     â”‚   max_tokens=100                     â”‚                â”‚
    â”‚     â”‚ )                                    â”‚                â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚                    â”‚                                        â”‚
    â”‚                    â–¼                                        â”‚
    â”‚  2. Message Flow (Same as ChatOpenAI):                      â”‚
    â”‚     [HumanMessage("What are embeddings?")]                  â”‚
    â”‚                    â”‚                                        â”‚
    â”‚                    â–¼                                        â”‚
    â”‚     llm.invoke(messages)                                    â”‚
    â”‚                    â”‚                                        â”‚
    â”‚                    â–¼                                        â”‚
    â”‚     AIMessage(content="Embeddings are...")                  â”‚
    â”‚                                                             â”‚
    â”‚  Provider Interchangeability:                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚  â”‚  ChatOpenAI    â”‚         â”‚ ChatAnthropic  â”‚              â”‚
    â”‚  â”‚  (GPT models)  â”‚   â†â†’    â”‚ (Claude models)â”‚              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚         â†“                            â†“                      â”‚
    â”‚    Same .invoke() interface                                 â”‚
    â”‚    Same message types                                       â”‚
    â”‚    Same response format                                     â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Identical interface to ChatOpenAI              â”‚
    â”‚  âœ… Benefit: Seamless provider switching                    â”‚
    â”‚  âœ… Benefit: No code changes needed for migration           â”‚
    â”‚  âœ… Benefit: Multi-provider fallback support                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 2: ChatAnthropic Basic Usage")

    _, has_anthropic = check_api_keys()
    if not has_anthropic:
        print("âš ï¸  ANTHROPIC_API_KEY not found - skipping demo")
        print("Set ANTHROPIC_API_KEY in .env to run this demo")
        return

    from langchain_anthropic import ChatAnthropic

    # Initialize chat model
    llm = ChatAnthropic(
        model="claude-3-5-haiku-20241022",  # using cheaper model for demo
        temperature=0.7,
        max_tokens=100,
    )

    print("## Model Configuration:")
    print(f"Model: {llm.model}")
    print(f"Temperature: {llm.temperature}")
    print(f"Max Tokens: {llm.max_tokens}")

    # Single message
    print("\n## Single Message:")
    messages = [HumanMessage(content="What are embeddings in one sentence?")]
    response = llm.invoke(messages)
    print(f"Response: {response.content}")

    # Multi-turn conversation
    print("\n## Multi-Turn Conversation:")
    conversation = [
        SystemMessage(content="You are a concise Python expert"),
        HumanMessage(content="How do I read a file?"),
    ]
    response = llm.invoke(conversation)
    print(f"Response: {response.content[:200]}...")


# endregion

# region Demo 3: Temperature and Creativity Control


@requires_openai
def demo_temperature_control() -> None:
    """
    demonstrate how temperature affects output

    Temperature Control Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       Temperature: Controlling Output Randomness            â”‚
    â”‚                                                             â”‚
    â”‚  Same Prompt, Different Temperatures:                       â”‚
    â”‚                                                             â”‚
    â”‚  Temperature 0.0 (Deterministic):                           â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚     â”‚ "AI-powered headphones use advanced  â”‚                â”‚
    â”‚     â”‚  noise cancellation. They adapt to   â”‚                â”‚
    â”‚     â”‚  your listening environment."        â”‚                â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚     â€¢ Most likely tokens chosen every time                  â”‚
    â”‚     â€¢ Consistent, repeatable responses                      â”‚
    â”‚     â€¢ Best for: factual tasks, data extraction              â”‚
    â”‚                                                             â”‚
    â”‚  Temperature 0.5 (Balanced):                                â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚     â”‚ "These AI headphones deliver smart   â”‚                â”‚
    â”‚     â”‚  noise cancellation with adaptive    â”‚                â”‚
    â”‚     â”‚  audio optimization."                â”‚                â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚     â€¢ Moderate randomness in token selection                â”‚
    â”‚     â€¢ More variety, still coherent                          â”‚
    â”‚     â€¢ Best for: general chat, recommendations               â”‚
    â”‚                                                             â”‚
    â”‚  Temperature 1.0 (Creative):                                â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚     â”‚ "Experience revolutionary AI audio   â”‚                â”‚
    â”‚     â”‚  that learns your preferences and    â”‚                â”‚
    â”‚     â”‚  transforms your listening journey." â”‚                â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚     â€¢ High randomness in token selection                    â”‚
    â”‚     â€¢ Most creative and varied                              â”‚
    â”‚     â€¢ Best for: creative writing, brainstorming             â”‚
    â”‚                                                             â”‚
    â”‚  Temperature Scale:                                         â”‚
    â”‚  0.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0.5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1.0 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2.0           â”‚
    â”‚  Deterministic   Balanced       Creative      Chaotic       â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Fine-tune creativity vs consistency            â”‚
    â”‚  âœ… Benefit: Task-specific optimization                     â”‚
    â”‚  âœ… Benefit: Control output variability                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 3: Temperature and Creativity Control")


    from langchain_openai import ChatOpenAI

    prompt = "Write a two-sentence product description for AI-powered headphones"
    messages = [HumanMessage(content=prompt)]

    print(f"Prompt: {prompt}\n")

    temperatures = [0.0, 0.5, 1.0]
    for temp in temperatures:
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=temp, max_tokens=100)
        response = llm.invoke(messages)

        print(f"Temperature {temp}:")
        print(f"â†’ {response.content}\n")

    print("Notice how higher temperature produces more varied/creative responses")


# endregion

# region Demo 4: Streaming Responses


@requires_openai
def demo_streaming() -> None:
    """
    demonstrate real-time streaming responses

    Streaming Response Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Streaming: Real-Time Token-by-Token Delivery        â”‚
    â”‚                                                             â”‚
    â”‚  Non-Streaming (llm.invoke()):                              â”‚
    â”‚     User sends request                                      â”‚
    â”‚            â”‚                                                â”‚
    â”‚            â–¼                                                â”‚
    â”‚     â³ Wait for full response...                            â”‚
    â”‚            â”‚                                                â”‚
    â”‚            â–¼                                                â”‚
    â”‚     Complete response delivered at once                     â”‚
    â”‚     "RAG is Retrieval-Augmented Generation..."              â”‚
    â”‚                                                             â”‚
    â”‚  Streaming (llm.stream()):                                  â”‚
    â”‚     User sends request                                      â”‚
    â”‚            â”‚                                                â”‚
    â”‚            â–¼                                                â”‚
    â”‚     Token 1: "RAG"          â† Immediate                     â”‚
    â”‚     Token 2: " is"          â† 50ms                          â”‚
    â”‚     Token 3: " Retrieval"   â† 100ms                         â”‚
    â”‚     Token 4: "-Augmented"   â† 150ms                         â”‚
    â”‚     Token 5: " Generation"  â† 200ms                         â”‚
    â”‚     ...                                                     â”‚
    â”‚                                                             â”‚
    â”‚  Implementation:                                            â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚     â”‚ for chunk in llm.stream(messages):   â”‚                â”‚
    â”‚     â”‚     print(chunk.content, end="")     â”‚                â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚                                                             â”‚
    â”‚  User Experience Comparison:                                â”‚
    â”‚  Non-Streaming:  â³â³â³â³ â†’ Full response                  â”‚
    â”‚  Streaming:      R â†’ RA â†’ RAG â†’ RAG is â†’ ...                â”‚
    â”‚                  â†‘ Feels faster, more interactive           â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Better perceived performance                   â”‚
    â”‚  âœ… Benefit: User sees progress immediately                 â”‚
    â”‚  âœ… Benefit: Can stop generation early                      â”‚
    â”‚  âœ… Benefit: Improved UX for long responses                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 4: Streaming Responses")


    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, streaming=True)

    messages = [
        SystemMessage(content="You are a helpful assistant"),
        HumanMessage(content="Explain what RAG is in 2-3 sentences"),
    ]

    print("## Streaming Response (tokens arrive in real-time):\n")
    for chunk in llm.stream(messages):
        print(chunk.content, end="", flush=True)
    print("\n")

    print("âœ“ Streaming provides better UX for interactive applications")


# endregion

# region Demo 5: Provider Switching


def demo_provider_switching() -> None:
    """
    demonstrate switching between providers seamlessly

    Provider Switching Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      Seamless Provider Switching: Same Interface            â”‚
    â”‚                                                             â”‚
    â”‚  Unified Message Interface:                                 â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
    â”‚     â”‚ messages = [                       â”‚                  â”‚
    â”‚     â”‚   SystemMessage("You are..."),     â”‚                  â”‚
    â”‚     â”‚   HumanMessage("What are...?")     â”‚                  â”‚
    â”‚     â”‚ ]                                  â”‚                  â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
    â”‚                    â”‚                                        â”‚
    â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”‚
    â”‚         â”‚                     â”‚                             â”‚
    â”‚         â–¼                     â–¼                             â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
    â”‚  â”‚  OpenAI    â”‚        â”‚ Anthropic  â”‚                       â”‚
    â”‚  â”‚  GPT-3.5   â”‚        â”‚ Claude 3.5 â”‚                       â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â”‚
    â”‚        â”‚                     â”‚                              â”‚
    â”‚        â–¼                     â–¼                              â”‚
    â”‚  openai_llm.invoke()   anthropic_llm.invoke()               â”‚
    â”‚        â”‚                     â”‚                              â”‚
    â”‚        â–¼                     â–¼                              â”‚
    â”‚  AIMessage(...)         AIMessage(...)                      â”‚
    â”‚                                                             â”‚
    â”‚  Provider Comparison:                                       â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚  â”‚ Feature     â”‚ OpenAI       â”‚ Anthropic    â”‚              â”‚
    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤              â”‚
    â”‚  â”‚ Interface   â”‚ ChatOpenAI   â”‚ ChatAnthropicâ”‚              â”‚
    â”‚  â”‚ invoke()    â”‚ âœ“ Same       â”‚ âœ“ Same       â”‚              â”‚
    â”‚  â”‚ stream()    â”‚ âœ“ Same       â”‚ âœ“ Same       â”‚              â”‚
    â”‚  â”‚ Messages    â”‚ âœ“ Same types â”‚ âœ“ Same types â”‚              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚                                                             â”‚
    â”‚  Migration Example:                                         â”‚
    â”‚     # Before: OpenAI                                        â”‚
    â”‚     llm = ChatOpenAI(model="gpt-3.5-turbo")                 â”‚
    â”‚                                                             â”‚
    â”‚     # After: Anthropic (only 1 line changes!)               â”‚
    â”‚     llm = ChatAnthropic(model="claude-3-5-haiku-20241022")  â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Zero code changes for provider switch          â”‚
    â”‚  âœ… Benefit: A/B test different providers easily            â”‚
    â”‚  âœ… Benefit: Multi-provider applications                    â”‚
    â”‚  âœ… Benefit: Vendor independence                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 5: Provider Switching")

    has_openai, has_anthropic = check_api_keys()

    if not (has_openai and has_anthropic):
        print("âš ï¸  Both API keys needed for this demo")
        print("Set OPENAI_API_KEY and ANTHROPIC_API_KEY in .env")
        return

    from langchain_anthropic import ChatAnthropic
    from langchain_openai import ChatOpenAI

    # Same prompt for both providers
    messages = [
        SystemMessage(content="You are a concise AI expert"),
        HumanMessage(content="What are embeddings in one sentence?"),
    ]

    # OpenAI
    print("## OpenAI GPT-3.5:")
    openai_llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    openai_response = openai_llm.invoke(messages)
    print(f"â†’ {openai_response.content}")

    # Anthropic
    print("\n## Anthropic Claude:")
    anthropic_llm = ChatAnthropic(
        model="claude-3-5-haiku-20241022", temperature=0.7, max_tokens=100
    )
    anthropic_response = anthropic_llm.invoke(messages)
    print(f"â†’ {anthropic_response.content}")

    print("\nâœ“ Same interface, different providers - seamless switching!")


# endregion

# region Demo 6: Fallback Chains


def demo_fallback_chain() -> None:
    """
    demonstrate fallback from primary to secondary provider

    Fallback Chain Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        Fallback Chains: Resilient Multi-Provider Setup      â”‚
    â”‚                                                             â”‚
    â”‚  Request Flow with Fallback:                                â”‚
    â”‚                                                             â”‚
    â”‚     User Request                                            â”‚
    â”‚          â”‚                                                  â”‚
    â”‚          â–¼                                                  â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
    â”‚     â”‚ Primary: GPT-4  â”‚ â† Try first (high quality)          â”‚
    â”‚     â”‚ timeout=5s      â”‚                                     â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
    â”‚              â”‚                                              â”‚
    â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
    â”‚    â”‚                   â”‚                                    â”‚
    â”‚    â–¼                   â–¼                                    â”‚
    â”‚ Success?           Timeout/Error?                           â”‚
    â”‚    â”‚                   â”‚                                    â”‚
    â”‚    â”‚                   â–¼                                    â”‚
    â”‚    â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
    â”‚    â”‚          â”‚ Fallback: Claude â”‚ â† Different provider     â”‚
    â”‚    â”‚          â”‚ 3.5 Haiku        â”‚                          â”‚
    â”‚    â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
    â”‚    â”‚                   â”‚                                    â”‚
    â”‚    â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
    â”‚    â”‚          â”‚                 â”‚                           â”‚
    â”‚    â”‚          â–¼                 â–¼                           â”‚
    â”‚    â”‚       Success?         Error?                          â”‚
    â”‚    â”‚          â”‚                 â”‚                           â”‚
    â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â†’ Return response/error   â”‚
    â”‚                                                             â”‚
    â”‚  Implementation:                                            â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚     â”‚ primary = ChatOpenAI(...)            â”‚                â”‚
    â”‚     â”‚ fallback = ChatAnthropic(...)        â”‚                â”‚
    â”‚     â”‚                                      â”‚                â”‚
    â”‚     â”‚ llm = primary.with_fallbacks(        â”‚                â”‚
    â”‚     â”‚     [fallback]                       â”‚                â”‚
    â”‚     â”‚ )                                    â”‚                â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚                                                             â”‚
    â”‚  Fallback Scenarios:                                        â”‚
    â”‚  â€¢ Primary timeout â†’ Try fallback                           â”‚
    â”‚  â€¢ API rate limit â†’ Switch provider                         â”‚
    â”‚  â€¢ Model unavailable â†’ Use alternative                      â”‚
    â”‚  â€¢ Cost optimization â†’ Cheap primary, quality fallback      â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Increased reliability (99.9%+ uptime)          â”‚
    â”‚  âœ… Benefit: Vendor independence and resilience             â”‚
    â”‚  âœ… Benefit: Cost optimization strategies                   â”‚
    â”‚  âœ… Benefit: Graceful degradation                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 6: Fallback Chains")

    has_openai, has_anthropic = check_api_keys()

    if not (has_openai and has_anthropic):
        print("âš ï¸  Both API keys needed for this demo")
        print("Set OPENAI_API_KEY and ANTHROPIC_API_KEY in .env")
        return

    from langchain_anthropic import ChatAnthropic
    from langchain_openai import ChatOpenAI

    # Primary: GPT-4 (expensive, high quality)
    primary = ChatOpenAI(model="gpt-4", temperature=0.7, max_tokens=100, timeout=5)

    # Fallback: Claude (different provider)
    fallback = ChatAnthropic(
        model="claude-3-5-haiku-20241022", temperature=0.7, max_tokens=100
    )

    # Chain with fallback
    llm_with_fallback = primary.with_fallbacks([fallback])

    print("## Fallback Chain: GPT-4 â†’ Claude")
    print("Primary: GPT-4 (if it fails...)")
    print("Fallback: Claude-3.5-Haiku\n")

    messages = [HumanMessage(content="What are embeddings?")]

    try:
        response = llm_with_fallback.invoke(messages)
        print(f"Response: {response.content[:200]}...")
        print("\nâœ“ Request succeeded (primary or fallback)")
    except Exception as e:
        print(f"âŒ Both providers failed: {e}")


# endregion

# region Demo 7: Token Usage Tracking


@requires_openai
def demo_token_tracking() -> None:
    """
    demonstrate tracking token usage and costs

    Token Usage Tracking Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       Token Tracking: Cost Monitoring and Optimization      â”‚
    â”‚                                                             â”‚
    â”‚  Request with Callback Tracking:                            â”‚
    â”‚                                                             â”‚
    â”‚     with get_openai_callback() as cb:                       â”‚
    â”‚          â”‚                                                  â”‚
    â”‚          â–¼                                                  â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
    â”‚     â”‚ User Prompt (Input)         â”‚                         â”‚
    â”‚     â”‚ "Explain embeddings..."     â”‚                         â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     Tokenization: 15 tokens                                 â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
    â”‚     â”‚ LLM Processing              â”‚                         â”‚
    â”‚     â”‚ (GPT-3.5-turbo)             â”‚                         â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
    â”‚     â”‚ AI Response (Output)          â”‚                       â”‚
    â”‚     â”‚ "Embeddings are numerical..." â”‚                       â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     Tokenization: 85 tokens                                 â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     Callback Records:                                       â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
    â”‚     â”‚ Prompt tokens:     15             â”‚                   â”‚
    â”‚     â”‚ Completion tokens: 85             â”‚                   â”‚
    â”‚     â”‚ Total tokens:      100            â”‚                   â”‚
    â”‚     â”‚                                   â”‚                   â”‚
    â”‚     â”‚ Cost Calculation:                 â”‚                   â”‚
    â”‚     â”‚ Input:  15 Ã— $0.50/1M = $0.000008 â”‚                   â”‚
    â”‚     â”‚ Output: 85 Ã— $1.50/1M = $0.000128 â”‚                   â”‚
    â”‚     â”‚ Total:                  $0.000136 â”‚                   â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
    â”‚                                                             â”‚
    â”‚  Cost Management:                                           â”‚
    â”‚  â€¢ Track per-request costs                                  â”‚
    â”‚  â€¢ Set budget alerts ($X per day/month)                     â”‚
    â”‚  â€¢ Compare provider costs (GPT-3.5 vs GPT-4 vs Claude)      â”‚
    â”‚  â€¢ Optimize prompt length (fewer input tokens)              â”‚
    â”‚  â€¢ Cache common responses                                   â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Real-time cost monitoring                      â”‚
    â”‚  âœ… Benefit: Budget control and alerts                      â”‚
    â”‚  âœ… Benefit: Provider cost comparison                       â”‚
    â”‚  âœ… Benefit: Optimization insights                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 7: Token Usage Tracking")


    from langchain.callbacks import get_openai_callback
    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

    messages = [
        SystemMessage(content="You are a helpful assistant"),
        HumanMessage(content="Explain embeddings in simple terms"),
    ]

    print("## Tracking Token Usage with Callbacks:\n")

    with get_openai_callback() as cb:
        response = llm.invoke(messages)

        print(f"Response: {response.content[:100]}...\n")

        print("Token Usage:")
        print(f"  Prompt tokens: {cb.prompt_tokens}")
        print(f"  Completion tokens: {cb.completion_tokens}")
        print(f"  Total tokens: {cb.total_tokens}")
        print(f"  Total cost: ${cb.total_cost:.6f}")

        # Calculate cost breakdown
        print("\nCost Breakdown (GPT-3.5-turbo pricing):")
        print(f"  Input cost:  ${cb.prompt_tokens / 1_000_000 * 0.50:.6f}")
        print(f"  Output cost: ${cb.completion_tokens / 1_000_000 * 1.50:.6f}")


# endregion

# region Demo 8: LCEL Integration


@requires_openai
def demo_lcel_integration() -> None:
    """
    demonstrate LangChain Expression Language integration

    LCEL Integration Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       LCEL: Composable LLM Chains with Pipe Operator        â”‚
    â”‚                                                             â”‚
    â”‚  Chain Composition: Prompt | LLM | Parser                   â”‚
    â”‚                                                             â”‚
    â”‚     Input: {"concept": "embeddings"}                        â”‚
    â”‚          â”‚                                                  â”‚
    â”‚          â–¼                                                  â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
    â”‚     â”‚ ChatPromptTemplate          â”‚                         â”‚
    â”‚     â”‚ "Explain {concept} in       â”‚                         â”‚
    â”‚     â”‚  one sentence"              â”‚                         â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â”‚ Formatted prompt                           â”‚
    â”‚                â–¼                                            â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
    â”‚     â”‚ ChatOpenAI                  â”‚                         â”‚
    â”‚     â”‚ (gpt-3.5-turbo)             â”‚                         â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â”‚ AIMessage                                  â”‚
    â”‚                â–¼                                            â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
    â”‚     â”‚ StrOutputParser             â”‚                         â”‚
    â”‚     â”‚ Extract .content            â”‚                         â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     Output: "Embeddings are numerical..."                   â”‚
    â”‚                                                             â”‚
    â”‚  LCEL Syntax:                                               â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
    â”‚     â”‚ chain = (                            â”‚                â”‚
    â”‚     â”‚     template                         â”‚                â”‚
    â”‚     â”‚     | llm                            â”‚                â”‚
    â”‚     â”‚     | parser                         â”‚                â”‚
    â”‚     â”‚ )                                    â”‚                â”‚
    â”‚     â”‚                                      â”‚                â”‚
    â”‚     â”‚ result = chain.invoke(input)         â”‚                â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
    â”‚                                                             â”‚
    â”‚  Multiple Concepts in Batch:                                â”‚
    â”‚     concepts = ["embeddings", "RAG", "fine-tuning"]         â”‚
    â”‚          â”‚                                                  â”‚
    â”‚          â–¼                                                  â”‚
    â”‚     Same chain processes all three                          â”‚
    â”‚          â”‚                                                  â”‚
    â”‚          â–¼                                                  â”‚
    â”‚     Three concise explanations                              â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Clean, readable composition                    â”‚
    â”‚  âœ… Benefit: Reusable chain components                      â”‚
    â”‚  âœ… Benefit: Type-safe data flow                            â”‚
    â”‚  âœ… Benefit: Easy to extend and modify                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 8: LCEL Integration with LLMs")


    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    # Create a chain: prompt | llm | parser
    template = ChatPromptTemplate.from_messages(
        [
            ("system", "You are a concise AI expert"),
            ("human", "Explain {concept} in one sentence"),
        ]
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    parser = StrOutputParser()

    # LCEL chain composition
    chain = template | llm | parser

    print("## LCEL Chain: Prompt | LLM | Parser\n")

    concepts = ["embeddings", "RAG", "fine-tuning"]
    for concept in concepts:
        result = chain.invoke({"concept": concept})
        print(f"{concept:15s}: {result}")

    print("\nâœ“ LCEL makes it easy to compose LLMs with prompts and parsers")


# endregion


# region Demo Menu Configuration

DEMOS = [
    Demo("1", "OpenAI Integration", "openai LLM wrapper and streaming", demo_openai_integration, needs_api=True),
    Demo("2", "Anthropic Integration", "claude LLM wrapper", demo_anthropic_integration, needs_api=True),
    Demo("3", "Multi-Provider Switch", "switch between providers dynamically", demo_multi_provider, needs_api=True),
    Demo("4", "Streaming Responses", "real-time streaming output", demo_streaming, needs_api=True),
    Demo("5", "Token Usage Tracking", "monitor API costs", demo_token_usage, needs_api=True),
    Demo("6", "Custom LLM Parameters", "temperature and other settings", demo_custom_parameters, needs_api=True),
    Demo("7", "Error Handling", "graceful API error handling", demo_error_handling, needs_api=True),
]

# endregion


def main() -> None:
    """interactive demo runner"""
    has_openai, has_anthropic = check_api_keys()
    runner = MenuRunner(DEMOS, title="LLM Integration - Practical Examples", has_api=has_openai or has_anthropic)
    runner.run()
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nðŸ‘‹ Goodbye!")
