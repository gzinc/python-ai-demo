"""
Chains - Hands-On Practice (Requires API Keys)

Demonstrates real chain execution with OpenAI/Anthropic, LCEL patterns, error handling,
and debugging techniques with actual LLM calls.

Run: uv run python -m phase7_frameworks.01_langchain_basics.03_chains.practical
"""

from inspect import cleandoc

from langchain_core.messages import HumanMessage, SystemMessage

from phase7_frameworks.utils import (
    check_api_keys,
    print_section,
    requires_both_keys,
    requires_openai,
)


# region Demo 1: Basic LCEL Chain


@requires_openai
def demo_basic_lcel_chain() -> None:
    """
    demonstrate basic prompt â†’ llm â†’ parser chain

    Basic LCEL Chain Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           LCEL: Prompt | LLM | Parser Pipeline              â”‚
    â”‚                                                             â”‚
    â”‚  Three Concepts Processed Through Same Chain:               â”‚
    â”‚                                                             â”‚
    â”‚  Input 1: {"concept": "embeddings"}                         â”‚
    â”‚      â”‚                                                      â”‚
    â”‚      â–¼                                                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
    â”‚  â”‚ ChatPromptTemplate   â”‚ "Explain {concept} in one..."     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
    â”‚             â”‚ Formatted: "Explain embeddings in one..."     â”‚
    â”‚             â–¼                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
    â”‚  â”‚ ChatOpenAI (GPT-3.5) â”‚ Process prompt                    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
    â”‚             â”‚ AIMessage("Embeddings are...")                â”‚
    â”‚             â–¼                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
    â”‚  â”‚ StrOutputParser      â”‚ Extract .content                  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
    â”‚             â”‚                                               â”‚
    â”‚             â–¼                                               â”‚
    â”‚  Output: "Embeddings are numerical representations..."      â”‚
    â”‚                                                             â”‚
    â”‚  LCEL Syntax:                                               â”‚
    â”‚     chain = prompt | llm | parser                           â”‚
    â”‚                â†‘      â†‘      â†‘                              â”‚
    â”‚                â”‚      â”‚      â””â”€ Extract string              â”‚
    â”‚                â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€ Generate response           â”‚
    â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Format template               â”‚
    â”‚                                                             â”‚
    â”‚  Same chain processes: embeddings â†’ RAG â†’ LCEL             â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Pipe operator (|) chains components            â”‚
    â”‚  âœ… Benefit: Reusable pipeline for multiple inputs          â”‚
    â”‚  âœ… Benefit: Clean, readable composition                    â”‚
    â”‚  âœ… Benefit: Type-safe data flow                            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 1: Basic LCEL Chain (Prompt â†’ LLM â†’ Parser)")

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    # components
    prompt = ChatPromptTemplate.from_template("Explain {concept} in one sentence")
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    parser = StrOutputParser()

    # LCEL composition
    chain = prompt | llm | parser

    print("## Chain Structure: prompt | llm | parser\n")

    concepts = ["embeddings", "RAG", "LCEL"]
    for concept in concepts:
        result = chain.invoke({"concept": concept})
        print(f"{concept:12s}: {result}")

    print("\nâœ“ LCEL syntax is concise and readable")


# endregion

# region Demo 2: Multi-Message Prompt Chain


@requires_openai
def demo_multi_message_chain() -> None:
    """
    demonstrate chain with system + user messages

    Multi-Message Chain Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       Multi-Message Templates: System + User Pattern        â”‚
    â”‚                                                             â”‚
    â”‚  Input: {role: "Python expert", task: "Explain lists"}      â”‚
    â”‚      â”‚                                                      â”‚
    â”‚      â–¼                                                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
    â”‚  â”‚ ChatPromptTemplate.from_messages([    â”‚                  â”‚
    â”‚  â”‚   ("system", "You are a {role}"),     â”‚                  â”‚
    â”‚  â”‚   ("human", "{task}")                 â”‚                  â”‚
    â”‚  â”‚ ])                                    â”‚                  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
    â”‚                 â”‚                                           â”‚
    â”‚                 â–¼                                           â”‚
    â”‚  Formatted Messages:                                        â”‚
    â”‚  [SystemMessage("You are a Python expert"),                 â”‚
    â”‚   HumanMessage("Explain list comprehensions")]              â”‚
    â”‚                 â”‚                                           â”‚
    â”‚                 â–¼                                           â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
    â”‚  â”‚ ChatOpenAI processes   â”‚                                 â”‚
    â”‚  â”‚ with system context    â”‚                                 â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                 â”‚
    â”‚             â”‚                                               â”‚
    â”‚             â–¼                                               â”‚
    â”‚  Response tailored to role (Python expert voice)            â”‚
    â”‚                                                             â”‚
    â”‚  Role Switching Example:                                    â”‚
    â”‚  â€¢ role="Python expert" â†’ Technical, code-focused           â”‚
    â”‚  â€¢ role="data scientist" â†’ Statistical, data-focused        â”‚
    â”‚  â€¢ Same template, different personalities                   â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: System message sets assistant personality      â”‚
    â”‚  âœ… Benefit: Reusable template for different roles          â”‚
    â”‚  âœ… Benefit: Consistent behavior across sessions            â”‚
    â”‚  âœ… Benefit: Separate system context from user input        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 2: Multi-Message Prompt Chain")

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    # system + user message template
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a concise {role}"),
        ("human", "{task}")
    ])

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    parser = StrOutputParser()

    chain = prompt | llm | parser

    print("## Multi-Message Chain:\n")

    tasks = [
        {"role": "Python expert", "task": "Explain list comprehensions"},
        {"role": "data scientist", "task": "Explain embeddings"},
    ]

    for task_data in tasks:
        result = chain.invoke(task_data)
        print(f"Role: {task_data['role']}")
        print(f"Task: {task_data['task']}")
        print(f"Response: {result}\n")


# endregion

# region Demo 3: Streaming Chain


@requires_openai
def demo_streaming_chain() -> None:
    """
    demonstrate real-time streaming with LCEL

    Streaming Chain Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Streaming: Progressive Token-by-Token Output        â”‚
    â”‚                                                             â”‚
    â”‚  Chain: prompt | llm (streaming=True) | parser              â”‚
    â”‚                                                             â”‚
    â”‚  for chunk in chain.stream(input):                          â”‚
    â”‚      â”‚                                                      â”‚
    â”‚      â–¼                                                      â”‚
    â”‚  Chunk 1: "RAG"           â† Immediate                       â”‚
    â”‚  Chunk 2: " is"           â† 50ms later                      â”‚
    â”‚  Chunk 3: " Retrieval"    â† 100ms later                     â”‚
    â”‚  Chunk 4: "-Augmented"    â† 150ms later                     â”‚
    â”‚  Chunk 5: " Generation"   â† 200ms later                     â”‚
    â”‚  ...                                                        â”‚
    â”‚                                                             â”‚
    â”‚  User Experience:                                           â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
    â”‚  â”‚ Time 0ms:   "R"                  â”‚                       â”‚
    â”‚  â”‚ Time 50ms:  "RA"                 â”‚                       â”‚
    â”‚  â”‚ Time 100ms: "RAG"                â”‚                       â”‚
    â”‚  â”‚ Time 150ms: "RAG is"             â”‚                       â”‚
    â”‚  â”‚ ...progressive display...        â”‚                       â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
    â”‚                                                             â”‚
    â”‚  vs. Non-Streaming:                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
    â”‚  â”‚ Time 0-500ms: â³ waiting...      â”‚                       â”‚
    â”‚  â”‚ Time 500ms:   Full response      â”‚                       â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Better perceived performance                   â”‚
    â”‚  âœ… Benefit: User sees immediate progress                   â”‚
    â”‚  âœ… Benefit: Can interrupt long generations                 â”‚
    â”‚  âœ… Benefit: Critical for chat UI experiences               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 3: Streaming Chain Execution")

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    prompt = ChatPromptTemplate.from_template(
        "Explain {concept} in 2-3 sentences"
    )
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, streaming=True)
    parser = StrOutputParser()

    chain = prompt | llm | parser

    print("## Streaming Response:\n")
    print("Concept: RAG")
    print("Response: ", end="", flush=True)

    for chunk in chain.stream({"concept": "RAG"}):
        print(chunk, end="", flush=True)

    print("\n\nâœ“ Streaming provides progressive output for better UX")


# endregion

# region Demo 4: Parallel Chain Execution


@requires_openai
def demo_parallel_chains() -> None:
    """
    demonstrate parallel chain execution with RunnableParallel

    Parallel Chain Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      RunnableParallel: Concurrent Independent Chains        â”‚
    â”‚                                                             â”‚
    â”‚  Input: {text: "AI is transforming software development"}   â”‚
    â”‚             â”‚                                               â”‚
    â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                        â”‚
    â”‚      â”‚             â”‚                                        â”‚
    â”‚      â–¼             â–¼                                        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
    â”‚  â”‚Summary  â”‚   â”‚Keywords â”‚  (Run simultaneously)            â”‚
    â”‚  â”‚Chain    â”‚   â”‚Chain    â”‚                                 â”‚
    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                 â”‚
    â”‚       â”‚             â”‚                                       â”‚
    â”‚       â”‚ "AI transforms software"                           â”‚
    â”‚       â”‚             â”‚ "AI, software, transform"            â”‚
    â”‚       â”‚             â”‚                                       â”‚
    â”‚       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                       â”‚
    â”‚              â”‚                                              â”‚
    â”‚              â–¼                                              â”‚
    â”‚  Output: {                                                  â”‚
    â”‚    "summary": "AI transforms software",                     â”‚
    â”‚    "keywords": "AI, software, transform"                    â”‚
    â”‚  }                                                          â”‚
    â”‚                                                             â”‚
    â”‚  Parallel Execution vs Sequential:                          â”‚
    â”‚                                                             â”‚
    â”‚  Sequential:  Summary â†’ Wait â†’ Keywords â†’ Wait             â”‚
    â”‚  Total time:  500ms + 500ms = 1000ms                        â”‚
    â”‚                                                             â”‚
    â”‚  Parallel:    Summary â”                                     â”‚
    â”‚               Keywordsâ”˜ (Both at once)                      â”‚
    â”‚  Total time:  max(500ms, 500ms) = 500ms                     â”‚
    â”‚                                                             â”‚
    â”‚  Implementation:                                            â”‚
    â”‚     RunnableParallel({                                      â”‚
    â”‚         "summary": summary_chain,                           â”‚
    â”‚         "keywords": keyword_chain                           â”‚
    â”‚     })                                                      â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: 2x speedup (or more) for independent chains    â”‚
    â”‚  âœ… Benefit: Single input, multiple analyses                â”‚
    â”‚  âœ… Benefit: Structured output with named results           â”‚
    â”‚  âœ… Benefit: Optimal for RAG with multiple retrievers       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 4: Parallel Chain Execution")

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnableParallel
    from langchain_openai import ChatOpenAI

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=50)
    parser = StrOutputParser()

    # define multiple chains
    summary_prompt = ChatPromptTemplate.from_template(
        "Summarize this in 5 words: {text}"
    )
    keyword_prompt = ChatPromptTemplate.from_template(
        "List 3 keywords for: {text}"
    )

    summary_chain = summary_prompt | llm | parser
    keyword_chain = keyword_prompt | llm | parser

    # parallel execution
    parallel_chain = RunnableParallel({
        "summary": summary_chain,
        "keywords": keyword_chain
    })

    print("## Parallel Chains: summary + keywords\n")

    text = "Artificial intelligence is transforming software development"
    result = parallel_chain.invoke({"text": text})

    print(f"Input: {text}\n")
    print(f"Summary:  {result['summary']}")
    print(f"Keywords: {result['keywords']}")

    print("\nâœ“ Parallel execution runs independent chains simultaneously")


# endregion

# region Demo 5: Passthrough Pattern for RAG


@requires_openai
def demo_passthrough_pattern() -> None:
    """
    demonstrate passthrough pattern for RAG-like workflows

    Passthrough Pattern (RAG):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       RunnablePassthrough: Preserve + Transform Pattern     â”‚
    â”‚                                                             â”‚
    â”‚  Question: "What are embeddings?"                           â”‚
    â”‚      â”‚                                                      â”‚
    â”‚      â–¼                                                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
    â”‚  â”‚ {                                  â”‚                     â”‚
    â”‚  â”‚   "context": Passthrough â†’ Retriever                     â”‚
    â”‚  â”‚   "question": Passthrough (unchanged)                    â”‚
    â”‚  â”‚ }                                  â”‚                     â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
    â”‚             â”‚                                               â”‚
    â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                                        â”‚
    â”‚      â”‚             â”‚                                        â”‚
    â”‚      â–¼             â–¼                                        â”‚
    â”‚  context:      question:                                    â”‚
    â”‚  "Retrieved:   "What are embeddings?"                       â”‚
    â”‚   Embeddings   (original preserved)                         â”‚
    â”‚   are vector                                               â”‚
    â”‚   representations"                                          â”‚
    â”‚      â”‚             â”‚                                        â”‚
    â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
    â”‚             â”‚                                               â”‚
    â”‚             â–¼                                               â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
    â”‚  â”‚ ChatPromptTemplate:              â”‚                       â”‚
    â”‚  â”‚ "Answer using context:           â”‚                       â”‚
    â”‚  â”‚  Context: {context}              â”‚                       â”‚
    â”‚  â”‚  Question: {question}"           â”‚                       â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
    â”‚             â”‚                                               â”‚
    â”‚             â–¼                                               â”‚
    â”‚  Formatted prompt with both values â†’ LLM â†’ Answer           â”‚
    â”‚                                                             â”‚
    â”‚  Why RunnablePassthrough?                                   â”‚
    â”‚  â€¢ Preserves original input unchanged                       â”‚
    â”‚  â€¢ Allows parallel transformations                          â”‚
    â”‚  â€¢ Critical for RAG: context (retrieved) + question (raw)   â”‚
    â”‚  â€¢ Enables complex data routing                             â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Preserve input alongside transformations       â”‚
    â”‚  âœ… Benefit: Foundation for RAG pipelines                   â”‚
    â”‚  âœ… Benefit: Clean data routing in chains                   â”‚
    â”‚  âœ… Benefit: Combine processed + original data              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 5: Passthrough Pattern (RAG Simulation)")

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnablePassthrough
    from langchain_openai import ChatOpenAI

    # simulate retriever (normally would query vector DB)
    def mock_retriever(query: str) -> str:
        """simulate document retrieval"""
        return f"Retrieved context: Embeddings are vector representations used in AI"

    # RAG prompt expecting context + question
    prompt = ChatPromptTemplate.from_template(cleandoc('''
        Answer the question using the context below.

        Context: {context}
        Question: {question}

        Answer:
    '''))

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    parser = StrOutputParser()

    # chain with passthrough pattern
    chain = {
        "context": RunnablePassthrough() | mock_retriever,  # process
        "question": RunnablePassthrough()                   # pass through
    } | prompt | llm | parser

    print("## RAG Pattern: context (retrieved) + question (passthrough)\n")

    question = "What are embeddings?"
    result = chain.invoke(question)

    print(f"Question: {question}")
    print(f"Answer: {result}")

    print("\nâœ“ Passthrough preserves original input alongside processed data")


# endregion

# region Demo 6: Fallback Chain


@requires_both_keys
def demo_fallback_chain() -> None:
    """
    demonstrate fallback between primary and secondary models

    Fallback Chain Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Fallback Chain: Primary â†’ Secondary Resilience      â”‚
    â”‚                                                             â”‚
    â”‚  Normal Execution (Primary succeeds):                       â”‚
    â”‚     Input â†’ GPT-4 (Primary)                                 â”‚
    â”‚                  â”‚                                          â”‚
    â”‚                  âœ“ Success                                  â”‚
    â”‚                  â”‚                                          â”‚
    â”‚                  â–¼                                          â”‚
    â”‚     Output: High-quality GPT-4 response                     â”‚
    â”‚                                                             â”‚
    â”‚  Fallback Execution (Primary fails):                        â”‚
    â”‚     Input â†’ GPT-4 (Primary)                                 â”‚
    â”‚                  â”‚                                          â”‚
    â”‚                  âœ— Timeout / Error / Rate limit             â”‚
    â”‚                  â”‚                                          â”‚
    â”‚                  â–¼                                          â”‚
    â”‚     Automatic switch to Claude Haiku (Fallback)             â”‚
    â”‚                  â”‚                                          â”‚
    â”‚                  âœ“ Success                                  â”‚
    â”‚                  â”‚                                          â”‚
    â”‚                  â–¼                                          â”‚
    â”‚     Output: Claude Haiku response (still gets answer!)      â”‚
    â”‚                                                             â”‚
    â”‚  Configuration:                                             â”‚
    â”‚     primary_chain.with_fallbacks([fallback_chain])          â”‚
    â”‚                                                             â”‚
    â”‚  Failure Scenarios Handled:                                 â”‚
    â”‚  â€¢ Network timeouts                                         â”‚
    â”‚  â€¢ API rate limits                                          â”‚
    â”‚  â€¢ Provider outages                                         â”‚
    â”‚  â€¢ Model unavailability                                     â”‚
    â”‚  â€¢ Token/billing issues                                     â”‚
    â”‚                                                             â”‚
    â”‚  Use Cases:                                                 â”‚
    â”‚  â€¢ Production reliability (99.9% uptime)                    â”‚
    â”‚  â€¢ Multi-provider redundancy                                â”‚
    â”‚  â€¢ Cost optimization (cheap fallback for failures)          â”‚
    â”‚  â€¢ Geographic failover                                      â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Zero downtime from single provider failures    â”‚
    â”‚  âœ… Benefit: Automatic failover (no manual intervention)    â”‚
    â”‚  âœ… Benefit: Cost-effective (fallback only when needed)     â”‚
    â”‚  âœ… Benefit: Production-grade reliability                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 6: Fallback Chain (Reliability Pattern)")

    from langchain_anthropic import ChatAnthropic
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    prompt = ChatPromptTemplate.from_template("Explain {concept} in one sentence")
    parser = StrOutputParser()

    # primary: GPT-4 (expensive, high quality)
    primary_llm = ChatOpenAI(model="gpt-4", temperature=0.7, max_tokens=100, timeout=5)
    primary_chain = prompt | primary_llm | parser

    # fallback: Claude Haiku (cheaper, fast)
    fallback_llm = ChatAnthropic(
        model="claude-3-5-haiku-20241022",
        temperature=0.7,
        max_tokens=100
    )
    fallback_chain = prompt | fallback_llm | parser

    # combine with fallback
    chain_with_fallback = primary_chain.with_fallbacks([fallback_chain])

    print("## Fallback Chain: GPT-4 â†’ Claude (if GPT-4 fails)\n")

    concepts = ["embeddings", "RAG"]
    for concept in concepts:
        try:
            result = chain_with_fallback.invoke({"concept": concept})
            print(f"{concept}: {result}")
        except Exception as e:
            print(f"{concept}: Both chains failed - {e}")

    print("\nâœ“ Fallback chains improve reliability and handle provider outages")


# endregion

# region Demo 7: Retry Configuration


@requires_openai
def demo_retry_chain() -> None:
    """
    demonstrate retry configuration for transient failures

    Retry Configuration Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      Retry Chain: Exponential Backoff with Jitter           â”‚
    â”‚                                                             â”‚
    â”‚  Attempt 1 (Immediate):                                     â”‚
    â”‚     Request â†’ LLM API                                       â”‚
    â”‚                â”‚                                            â”‚
    â”‚                âœ— Transient failure (network timeout)        â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     Wait: 1 second + random jitter (0-500ms)                â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚  Attempt 2:                                                 â”‚
    â”‚     Retry â†’ LLM API                                         â”‚
    â”‚                â”‚                                            â”‚
    â”‚                âœ— Still failing (rate limit)                 â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     Wait: 2 seconds + random jitter (0-1000ms)              â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚  Attempt 3 (Final):                                         â”‚
    â”‚     Retry â†’ LLM API                                         â”‚
    â”‚                â”‚                                            â”‚
    â”‚                âœ“ Success!                                   â”‚
    â”‚                â”‚                                            â”‚
    â”‚                â–¼                                            â”‚
    â”‚     Return response (user never knew about failures)        â”‚
    â”‚                                                             â”‚
    â”‚  Exponential Backoff Strategy:                              â”‚
    â”‚     Attempt 1: 0s wait (immediate)                          â”‚
    â”‚     Attempt 2: 1s + jitter                                  â”‚
    â”‚     Attempt 3: 2s + jitter                                  â”‚
    â”‚     Attempt 4: 4s + jitter (if configured)                  â”‚
    â”‚     Pattern: 2^(attempt-1) seconds                          â”‚
    â”‚                                                             â”‚
    â”‚  Why Add Random Jitter?                                     â”‚
    â”‚     Without: All clients retry at same time â†’ Thundering    â”‚
    â”‚              herd problem (overload server again)           â”‚
    â”‚     With:    Clients retry at random intervals â†’ Spread     â”‚
    â”‚              load, better success rate                      â”‚
    â”‚                                                             â”‚
    â”‚  Configuration:                                             â”‚
    â”‚     chain.with_retry(                                       â”‚
    â”‚         stop_after_attempt=3,                               â”‚
    â”‚         wait_exponential_jitter=True                        â”‚
    â”‚     )                                                       â”‚
    â”‚                                                             â”‚
    â”‚  Handles Transient Failures:                                â”‚
    â”‚  â€¢ Network timeouts (temporary)                             â”‚
    â”‚  â€¢ Rate limits (brief overload)                             â”‚
    â”‚  â€¢ Server 5xx errors (transient)                            â”‚
    â”‚  â€¢ Connection drops                                         â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Automatic recovery from transient errors       â”‚
    â”‚  âœ… Benefit: No manual retry logic needed                   â”‚
    â”‚  âœ… Benefit: Exponential backoff prevents server overload   â”‚
    â”‚  âœ… Benefit: Jitter prevents thundering herd                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 7: Retry Configuration")

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    prompt = ChatPromptTemplate.from_template("Explain {concept} in one sentence")
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    parser = StrOutputParser()

    # basic chain
    basic_chain = prompt | llm | parser

    # chain with retry (up to 3 attempts, exponential backoff)
    retry_chain = (prompt | llm | parser).with_retry(
        stop_after_attempt=3,
        wait_exponential_jitter=True
    )

    print("## Retry Chain Configuration:\n")
    print("Strategy: 3 attempts with exponential backoff + jitter")
    print("Exponential backoff with random jitter prevents thundering herd\n")

    print("Executing with retry protection:")
    result = retry_chain.invoke({"concept": "embeddings"})
    print(f"Result: {result}")

    print("\nâœ“ Retry configuration handles transient failures automatically")


# endregion

# region Demo 8: Batch Processing


@requires_openai
def demo_batch_processing() -> None:
    """
    demonstrate batch processing for efficiency

    Batch Processing Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚       Batch Processing: Parallel Execution for Efficiency   â”‚
    â”‚                                                             â”‚
    â”‚  Sequential Execution (Traditional):                        â”‚
    â”‚     Input 1: "embeddings" â†’ LLM â†’ Result 1 (500ms)          â”‚
    â”‚                                      â”‚                      â”‚
    â”‚                                      â–¼                      â”‚
    â”‚     Input 2: "RAG"        â†’ LLM â†’ Result 2 (500ms)          â”‚
    â”‚                                      â”‚                      â”‚
    â”‚                                      â–¼                      â”‚
    â”‚     Input 3: "fine-tuning"â†’ LLM â†’ Result 3 (500ms)          â”‚
    â”‚                                                             â”‚
    â”‚     Total time: 500ms + 500ms + 500ms = 1500ms              â”‚
    â”‚                                                             â”‚
    â”‚  Batch Execution (Parallel):                                â”‚
    â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
    â”‚     Input 1 â”€â”€â”€â”€â†’â”‚                  â”‚                       â”‚
    â”‚     Input 2 â”€â”€â”€â”€â†’â”‚  Batch Request   â”‚                       â”‚
    â”‚     Input 3 â”€â”€â”€â”€â†’â”‚  (Parallel)      â”‚                       â”‚
    â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
    â”‚                           â”‚                                 â”‚
    â”‚                     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”                           â”‚
    â”‚                     â”‚           â”‚                           â”‚
    â”‚              Result 1    Result 2    Result 3               â”‚
    â”‚              (500ms - all at once)                          â”‚
    â”‚                                                             â”‚
    â”‚     Total time: max(500ms, 500ms, 500ms) = 500ms            â”‚
    â”‚     Speedup: 1500ms / 500ms = 3x faster! âš¡                  â”‚
    â”‚                                                             â”‚
    â”‚  Implementation:                                            â”‚
    â”‚     # Sequential (slow)                                     â”‚
    â”‚     results = [chain.invoke(item) for item in items]        â”‚
    â”‚                                                             â”‚
    â”‚     # Batch (fast)                                          â”‚
    â”‚     results = chain.batch(items)                            â”‚
    â”‚                                                             â”‚
    â”‚  LangChain Optimization:                                    â”‚
    â”‚  â€¢ Batches multiple requests into single API call           â”‚
    â”‚  â€¢ Provider processes requests in parallel                  â”‚
    â”‚  â€¢ Results returned in same order as inputs                 â”‚
    â”‚  â€¢ Automatic concurrency management                         â”‚
    â”‚                                                             â”‚
    â”‚  When to Use Batch:                                         â”‚
    â”‚  â€¢ Processing multiple similar inputs                       â”‚
    â”‚  â€¢ Bulk data analysis (100s-1000s of items)                 â”‚
    â”‚  â€¢ Report generation (many sections)                        â”‚
    â”‚  â€¢ Dataset labeling/classification                          â”‚
    â”‚  â€¢ Translation of multiple texts                            â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: 2-10x speedup (depends on batch size)          â”‚
    â”‚  âœ… Benefit: Lower API costs (fewer round trips)            â”‚
    â”‚  âœ… Benefit: Maintains order (predictable results)          â”‚
    â”‚  âœ… Benefit: Automatic error handling per item              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 8: Batch Processing")

    import time

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    prompt = ChatPromptTemplate.from_template("Explain {concept} in one sentence")
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=50)
    parser = StrOutputParser()

    chain = prompt | llm | parser

    concepts = [
        {"concept": "embeddings"},
        {"concept": "RAG"},
        {"concept": "fine-tuning"}
    ]

    # sequential execution
    print("## Sequential vs Batch Execution:\n")
    print("Sequential (one at a time):")
    start = time.time()
    sequential_results = [chain.invoke(c) for c in concepts]
    sequential_time = time.time() - start

    # batch execution
    print(f"âœ“ Completed in {sequential_time:.2f}s\n")
    print("Batch (parallel):")
    start = time.time()
    batch_results = chain.batch(concepts)
    batch_time = time.time() - start
    print(f"âœ“ Completed in {batch_time:.2f}s\n")

    speedup = sequential_time / batch_time if batch_time > 0 else 1
    print(f"Speedup: {speedup:.1f}x faster with batch processing")

    print("\n## Results:\n")
    for i, (concept_dict, result) in enumerate(zip(concepts, batch_results), 1):
        print(f"{i}. {concept_dict['concept']:15s}: {result}")


# endregion

# region Demo 9: Debugging with Verbose Mode


@requires_openai
def demo_verbose_debugging() -> None:
    """
    demonstrate verbose mode for debugging

    Verbose Debugging Mode:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Verbose Mode: Chain Execution Transparency          â”‚
    â”‚                                                             â”‚
    â”‚  Normal Execution (verbose=False):                          â”‚
    â”‚     chain.invoke(input)                                     â”‚
    â”‚           â”‚                                                 â”‚
    â”‚           â–¼                                                 â”‚
    â”‚     Final result only                                       â”‚
    â”‚     (Black box - no visibility)                             â”‚
    â”‚                                                             â”‚
    â”‚  Verbose Execution (verbose=True):                          â”‚
    â”‚     chain.invoke(input, config={"verbose": True})           â”‚
    â”‚           â”‚                                                 â”‚
    â”‚           â–¼                                                 â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚     â”‚ Step 1: PromptTemplate              â”‚                 â”‚
    â”‚     â”‚   Input: {concept: "LCEL"}          â”‚                 â”‚
    â”‚     â”‚   Output: "Explain LCEL briefly"    â”‚                 â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚               â”‚                                             â”‚
    â”‚               â–¼                                             â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚     â”‚ Step 2: ChatOpenAI                  â”‚                 â”‚
    â”‚     â”‚   Model: gpt-3.5-turbo              â”‚                 â”‚
    â”‚     â”‚   Temperature: 0.7                  â”‚                 â”‚
    â”‚     â”‚   Tokens: 50 max                    â”‚                 â”‚
    â”‚     â”‚   Response: AIMessage(...)          â”‚                 â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚               â”‚                                             â”‚
    â”‚               â–¼                                             â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚     â”‚ Step 3: StrOutputParser             â”‚                 â”‚
    â”‚     â”‚   Input: AIMessage                  â”‚                 â”‚
    â”‚     â”‚   Output: "LCEL is..."              â”‚                 â”‚
    â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚               â”‚                                             â”‚
    â”‚               â–¼                                             â”‚
    â”‚     Final result with full execution trace                  â”‚
    â”‚                                                             â”‚
    â”‚  Debugging Information Shown:                               â”‚
    â”‚  â€¢ Each chain step execution                                â”‚
    â”‚  â€¢ Input/output at each stage                               â”‚
    â”‚  â€¢ Component types being executed                           â”‚
    â”‚  â€¢ Timing information                                       â”‚
    â”‚  â€¢ Token usage (if available)                               â”‚
    â”‚  â€¢ Error locations (when failures occur)                    â”‚
    â”‚                                                             â”‚
    â”‚  Use Cases:                                                 â”‚
    â”‚  â€¢ Chain not working as expected                            â”‚
    â”‚  â€¢ Need to understand data transformation                   â”‚
    â”‚  â€¢ Debugging complex multi-step chains                      â”‚
    â”‚  â€¢ Learning how LCEL works internally                       â”‚
    â”‚  â€¢ Performance optimization                                 â”‚
    â”‚  â€¢ Validation of chain structure                            â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: See exactly what each chain step does          â”‚
    â”‚  âœ… Benefit: Identify where failures occur                  â”‚
    â”‚  âœ… Benefit: Understand data transformations                â”‚
    â”‚  âœ… Benefit: Learning tool for LCEL internals               â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 9: Debugging with Verbose Mode")

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI

    prompt = ChatPromptTemplate.from_template("Explain {concept} briefly")
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=50)
    parser = StrOutputParser()

    chain = prompt | llm | parser

    print("## Verbose Mode (shows execution details):\n")

    result = chain.invoke(
        {"concept": "LCEL"},
        config={"verbose": True}
    )

    print(f"\nFinal Result: {result}")

    print("\nâœ“ Verbose mode helps debug chain execution flow")


# endregion

# region Demo 10: Custom Transformation


@requires_openai
def demo_custom_transformation() -> None:
    """
    demonstrate custom transformation with RunnableLambda

    Custom Transformation Pattern:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      RunnableLambda: Custom Transformations in Chains       â”‚
    â”‚                                                             â”‚
    â”‚  Standard Chain (No Custom Logic):                          â”‚
    â”‚     Prompt â†’ LLM â†’ StrOutputParser                          â”‚
    â”‚        â”‚      â”‚          â”‚                                  â”‚
    â”‚        â–¼      â–¼          â–¼                                  â”‚
    â”‚     Template  Model    String output                        â”‚
    â”‚                                                             â”‚
    â”‚  Chain with RunnableLambda:                                 â”‚
    â”‚     Prompt â†’ LLM â†’ StrOutputParser â†’ RunnableLambda         â”‚
    â”‚        â”‚      â”‚          â”‚                  â”‚               â”‚
    â”‚        â–¼      â–¼          â–¼                  â–¼               â”‚
    â”‚     Template  Model    "1. AI\n         Custom function     â”‚
    â”‚                        2. ML\n         transforms output    â”‚
    â”‚                        3. NLP"                              â”‚
    â”‚                                           â”‚                 â”‚
    â”‚                                           â–¼                 â”‚
    â”‚                                    "ðŸ“‹ SUMMARY:             â”‚
    â”‚                                     1. AI                   â”‚
    â”‚                                     2. ML                   â”‚
    â”‚                                     3. NLP"                 â”‚
    â”‚                                                             â”‚
    â”‚  Custom Transformation Function:                            â”‚
    â”‚     def custom_transform(text: str) -> str:                 â”‚
    â”‚         # Add formatting                                    â”‚
    â”‚         return f"ðŸ“‹ SUMMARY:\n{text.upper()}"               â”‚
    â”‚                                                             â”‚
    â”‚  Integration:                                               â”‚
    â”‚     chain = (                                               â”‚
    â”‚         prompt                                              â”‚
    â”‚         | llm                                               â”‚
    â”‚         | parser                                            â”‚
    â”‚         | RunnableLambda(custom_transform)                  â”‚
    â”‚     )                                                       â”‚
    â”‚                                                             â”‚
    â”‚  What RunnableLambda Can Do:                                â”‚
    â”‚  â€¢ Format output (add headers, styles, emojis)              â”‚
    â”‚  â€¢ Parse and restructure data                               â”‚
    â”‚  â€¢ Filter/validate results                                  â”‚
    â”‚  â€¢ Call external APIs or databases                          â”‚
    â”‚  â€¢ Apply business logic                                     â”‚
    â”‚  â€¢ Transform between formats (JSON â†’ CSV)                   â”‚
    â”‚  â€¢ Clean/sanitize output                                    â”‚
    â”‚  â€¢ Combine multiple results                                 â”‚
    â”‚                                                             â”‚
    â”‚  Use Cases:                                                 â”‚
    â”‚  â€¢ Format LLM output for specific UI needs                  â”‚
    â”‚  â€¢ Add metadata or timestamps                               â”‚
    â”‚  â€¢ Integrate with external systems                          â”‚
    â”‚  â€¢ Apply domain-specific transformations                    â”‚
    â”‚  â€¢ Validate and clean LLM responses                         â”‚
    â”‚                                                             â”‚
    â”‚  âœ… Benefit: Insert any Python function into chains         â”‚
    â”‚  âœ… Benefit: Full flexibility for custom logic              â”‚
    â”‚  âœ… Benefit: Composable with other LCEL components          â”‚
    â”‚  âœ… Benefit: Maintains streaming and async support          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Demo 10: Custom Transformation in Chain")

    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.runnables import RunnableLambda
    from langchain_openai import ChatOpenAI

    prompt = ChatPromptTemplate.from_template("List 3 {items}")
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7, max_tokens=100)
    parser = StrOutputParser()

    # custom transformation: convert to uppercase and add prefix
    def custom_transform(text: str) -> str:
        """add formatting to output"""
        return f"ðŸ“‹ SUMMARY:\n{text.upper()}"

    # chain with custom transformation
    chain = prompt | llm | parser | RunnableLambda(custom_transform)

    print("## Chain with Custom Transformation:\n")

    result = chain.invoke({"items": "AI concepts"})
    print(result)

    print("\nâœ“ RunnableLambda enables custom transformations in chains")


# endregion


def main() -> None:
    """run all practical demos"""
    print(cleandoc('''
        Chains - Practical Demos

        This module demonstrates real chain execution with LLM calls.
        Ensure you have API keys set in .env file.
    '''))

    has_openai, has_anthropic = check_api_keys()

    print("\n## API Key Status:")
    print(f"OPENAI_API_KEY: {'âœ“ Found' if has_openai else 'âœ— Missing'}")
    print(f"ANTHROPIC_API_KEY: {'âœ“ Found' if has_anthropic else 'âœ— Missing'}")

    if not (has_openai or has_anthropic):
        print("\nâŒ No API keys found!")
        print("Set at least one API key in .env to run demos:")
        print("  OPENAI_API_KEY=your-key-here")
        print("  ANTHROPIC_API_KEY=your-key-here")
        return

    demo_basic_lcel_chain()
    demo_multi_message_chain()
    demo_streaming_chain()
    demo_parallel_chains()
    demo_passthrough_pattern()
    demo_fallback_chain()
    demo_retry_chain()
    demo_batch_processing()
    demo_verbose_debugging()
    demo_custom_transformation()

    print("\n" + "=" * 70)
    print("  Practical demos complete! You've mastered chain composition.")
    print("=" * 70)


if __name__ == "__main__":
    main()
