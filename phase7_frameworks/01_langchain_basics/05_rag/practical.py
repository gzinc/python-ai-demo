"""
RAG (Retrieval-Augmented Generation) - Practical Examples

This module demonstrates real RAG implementations with OpenAI and vector stores.
Requires OPENAI_API_KEY environment variable.

Note: Demos use persistent ChromaDB storage in ./chroma_rag_db/
First run creates embeddings via API. Subsequent runs load from disk.

Run with: uv run python -m phase7_frameworks.01_langchain_basics.05_rag.practical
"""

import tempfile
from inspect import cleandoc
from pathlib import Path

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from phase7_frameworks.utils import check_api_keys, print_section, requires_openai


# region Helper Functions
def create_sample_documents() -> list[Document]:
    """create sample documents for RAG demonstrations"""
    docs = [
        Document(
            page_content=cleandoc(
                """
            Python is a high-level, interpreted programming language known for its
            simplicity and readability. It was created by Guido van Rossum and first
            released in 1991. Python emphasizes code readability with significant
            whitespace and supports multiple programming paradigms including procedural,
            object-oriented, and functional programming.
            """
            ),
            metadata={"source": "python_intro.txt", "topic": "programming"},
        ),
        Document(
            page_content=cleandoc(
                """
            Machine learning is a subset of artificial intelligence that enables
            computers to learn from data without being explicitly programmed. It uses
            algorithms that iteratively learn from data, allowing computers to find
            hidden insights. Machine learning models improve their performance over time
            as they are exposed to more data.
            """
            ),
            metadata={"source": "ml_basics.txt", "topic": "ai"},
        ),
        Document(
            page_content=cleandoc(
                """
            Natural Language Processing (NLP) is a branch of AI that helps computers
            understand, interpret, and manipulate human language. NLP draws from many
            disciplines including computer science and linguistics. Modern NLP uses
            machine learning, especially deep learning models like transformers, to
            achieve human-like language understanding.
            """
            ),
            metadata={"source": "nlp_guide.txt", "topic": "ai"},
        ),
        Document(
            page_content=cleandoc(
                """
            Vector databases are specialized databases designed to store and query
            high-dimensional vectors efficiently. They are essential for AI applications
            that use embeddings, such as semantic search and recommendation systems.
            Vector databases use specialized indexing techniques like HNSW (Hierarchical
            Navigable Small World) to enable fast similarity search.
            """
            ),
            metadata={"source": "vector_db.txt", "topic": "database"},
        ),
        Document(
            page_content=cleandoc(
                """
            Retrieval-Augmented Generation (RAG) is a technique that combines information
            retrieval with text generation. RAG first retrieves relevant documents from a
            knowledge base, then uses those documents as context for an LLM to generate
            accurate, grounded responses. This approach reduces hallucinations and enables
            LLMs to access current, domain-specific information.
            """
            ),
            metadata={"source": "rag_explained.txt", "topic": "ai"},
        ),
    ]
    return docs


def get_or_create_vectorstore(
    persist_directory: str = "./chroma_rag_db",
    collection_name: str = "phase7_rag_demos",
) -> Chroma:
    """
    get or create persistent vectorstore for RAG demos

    On first run: creates embeddings via API and persists to disk
    On subsequent runs: loads from disk (zero API calls)

    Args:
        persist_directory: directory for ChromaDB persistence
        collection_name: name for the collection

    Returns:
        Chroma vectorstore instance
    """

    #  If you want to use local embeddings in Phase 7 (no API key needed):

    # from langchain_community.embeddings import HuggingFaceEmbeddings
    # embeddings = HuggingFaceEmbeddings(
    #     model_name="sentence-transformers/all-MiniLM-L6-v2"
    # )

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

    # try loading existing collection
    try:
        vectorstore = Chroma(
            persist_directory=persist_directory,
            collection_name=collection_name,
            embedding_function=embeddings,
        )
        # verify collection has data
        if vectorstore._collection.count() > 0:
            print(f"   âœ“ loaded existing collection ({vectorstore._collection.count()} docs)")
            return vectorstore
    except Exception:
        pass  # collection doesn't exist yet

    # create new collection with embeddings
    print("   âš¡ creating new collection (one-time embedding API calls)...")
    documents = create_sample_documents()
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        persist_directory=persist_directory,
        collection_name=collection_name,
    )
    print(f"   âœ“ created and persisted {len(documents)} documents")
    return vectorstore


# endregion


# region Demo 1: Basic RAG with ChromaDB
@requires_openai
def demo_basic_rag() -> None:
    """
    demonstrate basic RAG pipeline with ChromaDB

    Basic RAG Flow:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    Basic RAG Pipeline                       â”‚
    â”‚                                                             â”‚
    â”‚  User Question                                              â”‚
    â”‚       â”‚                                                     â”‚
    â”‚       â–¼                                                     â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
    â”‚  â”‚  Embedding   â”‚ (convert question to vector)              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
    â”‚         â”‚                                                   â”‚
    â”‚         â–¼                                                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
    â”‚  â”‚  VectorDB    â”‚ (similarity search)                       â”‚
    â”‚  â”‚   Search     â”‚                                           â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
    â”‚         â”‚                                                   â”‚
    â”‚         â–¼                                                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
    â”‚  â”‚  Retrieved   â”‚ (top-k most similar docs)                 â”‚
    â”‚  â”‚  Documents   â”‚                                           â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
    â”‚         â”‚                                                   â”‚
    â”‚         â–¼                                                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
    â”‚  â”‚   Format     â”‚ (combine docs into context)               â”‚
    â”‚  â”‚   Context    â”‚                                           â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
    â”‚         â”‚                                                   â”‚
    â”‚         â–¼                                                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
    â”‚  â”‚  LLM Prompt  â”‚ (context + question)                      â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
    â”‚         â”‚                                                   â”‚
    â”‚         â–¼                                                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
    â”‚  â”‚     LLM      â”‚ (generate grounded answer)                â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
    â”‚         â”‚                                                   â”‚
    â”‚         â–¼                                                   â”‚
    â”‚     Answer                                                  â”‚
    â”‚                                                             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    This is the foundation of RAG: retrieve relevant context, then
    generate answers grounded in that context.
    """
    print_section("Basic RAG Pipeline")

    # initialize LLM
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    # load persistent vector store
    print("\nğŸ’¾ loading vector store...")
    vectorstore = get_or_create_vectorstore()

    # create retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    # create RAG prompt
    template = cleandoc(
        """
        answer the question based only on the following context:

        {context}

        question: {question}

        answer:
        """
    )
    prompt = ChatPromptTemplate.from_template(template)

    # create RAG chain using LCEL
    def format_docs(docs: list[Document]) -> str:
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # test with questions
    questions = [
        "What is Python and when was it created?",
        "How does machine learning work?",
        "What is RAG and why is it useful?",
    ]

    for question in questions:
        print(f"\nâ“ question: {question}")
        response = rag_chain.invoke(question)
        print(f"ğŸ’¬ answer: {response}")


# endregion


# region Demo 2: Text Chunking Strategies
def demo_text_chunking() -> None:
    """
    demonstrate different text chunking approaches

    Text Chunking with Overlap:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              Original Document                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
    â”‚  â”‚  The quick brown fox jumps over the lazy   â”‚         â”‚
    â”‚  â”‚  dog. The dog was sleeping under a tree.   â”‚         â”‚
    â”‚  â”‚  The tree provided shade from the sun.     â”‚         â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
    â”‚                      â”‚                                  â”‚
    â”‚                      â–¼                                  â”‚
    â”‚         RecursiveCharacterTextSplitter                  â”‚
    â”‚         (chunk_size=50, overlap=10)                     â”‚
    â”‚                      â”‚                                  â”‚
    â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
    â”‚           â–¼                     â–¼                       â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚  â”‚ Chunk 1:        â”‚   â”‚ Chunk 2:        â”‚              â”‚
    â”‚  â”‚ "The quick...   â”‚   â”‚ "...lazy dog.   â”‚              â”‚
    â”‚  â”‚  lazy dog."     â”‚   â”‚  The dog was..."â”‚              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚         â”‚  overlap          â”‚                           â”‚
    â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
    â”‚                 â–¼                                       â”‚
    â”‚         "...lazy dog." (shared)                         â”‚
    â”‚                                                         â”‚
    â”‚  Benefits of overlap:                                   â”‚
    â”‚  â€¢ Preserves context across chunk boundaries            â”‚
    â”‚  â€¢ Prevents splitting sentences/concepts                â”‚
    â”‚  â€¢ Improves retrieval accuracy                          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Text Chunking Strategies")

    sample_text = cleandoc(
        """
        Artificial intelligence (AI) is revolutionizing technology across industries.
        Machine learning, a subset of AI, enables computers to learn from data without
        explicit programming.

        Deep learning, which uses neural networks with multiple layers, has achieved
        remarkable success in computer vision and natural language processing. These
        models can process vast amounts of data to recognize patterns and make predictions.

        Natural Language Processing (NLP) focuses on the interaction between computers
        and human language. Modern NLP systems use transformer architectures like BERT
        and GPT to achieve human-like language understanding and generation capabilities.

        The field continues to evolve rapidly, with new breakthroughs in reasoning,
        multimodal understanding, and efficient model training techniques.
        """
    )

    print(f"ğŸ“– original text length: {len(sample_text)} characters")

    # strategy 1: recursive character splitter (recommended)
    print("\nâœ‚ï¸ strategy 1: recursive character splitter")
    splitter1 = RecursiveCharacterTextSplitter(
        chunk_size=200, chunk_overlap=50, separators=["\n\n", "\n", " ", ""]
    )
    chunks1 = splitter1.split_text(sample_text)
    print(f"   created {len(chunks1)} chunks")
    for i, chunk in enumerate(chunks1[:2], 1):
        print(f"   chunk {i}: {chunk[:80]}...")

    # strategy 2: larger chunks for more context
    print("\nâœ‚ï¸ strategy 2: larger chunks (more context)")
    splitter2 = RecursiveCharacterTextSplitter(
        chunk_size=400, chunk_overlap=100, separators=["\n\n", "\n", " ", ""]
    )
    chunks2 = splitter2.split_text(sample_text)
    print(f"   created {len(chunks2)} chunks")
    for i, chunk in enumerate(chunks2, 1):
        print(f"   chunk {i} length: {len(chunk)} chars")

    print("\nğŸ“Š comparison:")
    print(f"   small chunks (200): {len(chunks1)} chunks, better precision")
    print(f"   large chunks (400): {len(chunks2)} chunks, more context")


# endregion


# region Demo 3: Similarity Search Methods
@requires_openai
def demo_similarity_search() -> None:
    """
    demonstrate different similarity search methods

    Similarity Search Methods:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           Vector Similarity Search Methods                   â”‚
    â”‚                                                              â”‚
    â”‚  1. Basic Similarity Search (Cosine Distance):               â”‚
    â”‚     Query Vector                                             â”‚
    â”‚          â”‚                                                   â”‚
    â”‚          â–¼                                                   â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
    â”‚     â”‚VectorDB â”‚ â†’ Compare with all vectors                   â”‚
    â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                              â”‚
    â”‚          â”‚                                                   â”‚
    â”‚          â–¼                                                   â”‚
    â”‚     Return top-k most similar                                â”‚
    â”‚                                                              â”‚
    â”‚  2. Similarity Search with Scores:                           â”‚
    â”‚     Query Vector                                             â”‚
    â”‚          â”‚                                                   â”‚
    â”‚          â–¼                                                   â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
    â”‚     â”‚VectorDB â”‚ â†’ Calculate similarity scores                â”‚
    â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                              â”‚
    â”‚          â”‚                                                   â”‚
    â”‚          â–¼                                                   â”‚
    â”‚     [(Doc1, 0.95), (Doc2, 0.87), (Doc3, 0.82)]               â”‚
    â”‚                                                              â”‚
    â”‚  3. MMR (Max Marginal Relevance):                            â”‚
    â”‚     Query Vector                                             â”‚
    â”‚          â”‚                                                   â”‚
    â”‚          â–¼                                                   â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚
    â”‚     â”‚VectorDB â”‚ â†’ Fetch top-k candidates (fetch_k=4)         â”‚
    â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                              â”‚
    â”‚          â”‚                                                   â”‚
    â”‚          â–¼                                                   â”‚
    â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                      â”‚
    â”‚     â”‚  Diversity      â”‚ â†’ Balance relevance vs diversity     â”‚
    â”‚     â”‚  Reranking      â”‚                                      â”‚
    â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                      â”‚
    â”‚          â”‚                                                   â”‚
    â”‚          â–¼                                                   â”‚
    â”‚     Return diverse top-k (k=2)                               â”‚
    â”‚                                                              â”‚
    â”‚  Trade-offs:                                                 â”‚
    â”‚  â€¢ Basic: Fastest, most relevant, but may be redundant       â”‚
    â”‚  â€¢ With Scores: Same as basic + confidence metric            â”‚
    â”‚  â€¢ MMR: Slower, but prevents redundant similar results       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Similarity Search Methods")

    # load persistent vector store
    vectorstore = get_or_create_vectorstore()

    query = "What is artificial intelligence and machine learning?"

    # method 1: basic similarity search
    print(f"ğŸ” query: '{query}'")
    print("\nğŸ“Š method 1: similarity search (top 2)")
    results = vectorstore.similarity_search(query, k=2)
    for i, doc in enumerate(results, 1):
        print(f"   {i}. {doc.page_content[:80]}...")
        print(f"      source: {doc.metadata.get('source', 'unknown')}")

    # method 2: similarity search with scores
    print("\nğŸ“Š method 2: similarity search with scores")
    results_with_scores = vectorstore.similarity_search_with_score(query, k=2)
    for i, (doc, score) in enumerate(results_with_scores, 1):
        print(f"   {i}. (score: {score:.4f}) {doc.page_content[:60]}...")

    # method 3: mmr (max marginal relevance) for diversity
    print("\nğŸ“Š method 3: MMR (max marginal relevance)")
    print("   (balances relevance with diversity)")
    mmr_results = vectorstore.max_marginal_relevance_search(query, k=2, fetch_k=4)
    for i, doc in enumerate(mmr_results, 1):
        print(f"   {i}. {doc.page_content[:80]}...")


# endregion


# region Demo 4: Metadata Filtering
@requires_openai
def demo_metadata_filtering() -> None:
    """
    demonstrate retrieval with metadata filters

    Metadata Filtering Flow:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              Metadata Filtering in Vector Search             â”‚
    â”‚                                                              â”‚
    â”‚  Without Filter (search all documents):                      â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚  â”‚  VectorDB (all topics)                     â”‚              â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”        â”‚              â”‚
    â”‚  â”‚  â”‚ AI â”‚ â”‚ DB â”‚ â”‚ AI â”‚ â”‚Codeâ”‚ â”‚ AI â”‚        â”‚              â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”˜        â”‚              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚           â”‚                                                  â”‚
    â”‚           â–¼                                                  â”‚
    â”‚  Query â†’ Similarity Search â†’ Top 3 (mixed topics)            â”‚
    â”‚                                                              â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€               â”‚
    â”‚                                                              â”‚
    â”‚  With Filter (topic='ai' only):                              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
    â”‚  â”‚  VectorDB (filtered view)                  â”‚              â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”    â”‚              â”‚
    â”‚  â”‚  â”‚ AI â”‚    âœ“     â”‚ AI â”‚    âœ“     â”‚ AI â”‚    â”‚              â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”˜    â”‚              â”‚
    â”‚  â”‚         â”Œâ”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”             â”‚              â”‚
    â”‚  â”‚         â”‚ DB â”‚    âœ—     â”‚Codeâ”‚    âœ—        â”‚              â”‚
    â”‚  â”‚         â””â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”˜             â”‚              â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
    â”‚           â”‚                                                  â”‚
    â”‚           â–¼                                                  â”‚
    â”‚  Query â†’ Filter â†’ Similarity Search â†’ Top 3 (AI only)        â”‚
    â”‚                                                              â”‚
    â”‚  Benefits:                                                   â”‚
    â”‚  â€¢ Faster: Searches smaller subset                           â”‚
    â”‚  â€¢ Precise: Only relevant categories                         â”‚
    â”‚  â€¢ Flexible: Time-based, user-based, category filtering      â”‚
    â”‚  â€¢ Multi-tenant: Isolate data by tenant ID                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Metadata Filtering")

    # load persistent vector store
    vectorstore = get_or_create_vectorstore()

    query = "Tell me about technology"

    # search without filter
    print(f"ğŸ” query: '{query}'")
    print("\nğŸ“Š without filter (all topics):")
    results = vectorstore.similarity_search(query, k=3)
    for i, doc in enumerate(results, 1):
        topic = doc.metadata.get("topic", "unknown")
        print(f"   {i}. [{topic}] {doc.page_content[:60]}...")

    # search with metadata filter (only AI topic)
    print("\nğŸ“Š with filter (topic='ai' only):")
    filtered_results = vectorstore.similarity_search(query, k=3, filter={"topic": "ai"})
    for i, doc in enumerate(filtered_results, 1):
        topic = doc.metadata.get("topic", "unknown")
        print(f"   {i}. [{topic}] {doc.page_content[:60]}...")

    print("\nğŸ’¡ metadata filtering:")
    print("   â€¢ narrows search scope before similarity calculation")
    print("   â€¢ useful for multi-tenant applications")
    print("   â€¢ enables time-based or category-based filtering")


# endregion


# region Demo 5: Document Loading from Files
def demo_document_loading() -> None:
    """
    demonstrate loading documents from files

    Document Loading Pipeline:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              File â†’ RAG Pipeline                             â”‚
    â”‚                                                              â”‚
    â”‚  Filesystem                                                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
    â”‚  â”‚ file1.txt  â”‚  â”‚ file2.txt  â”‚   â”‚ file3.pdf  â”‚             â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚
    â”‚        â”‚               â”‚                â”‚                    â”‚
    â”‚        â–¼               â–¼                â–¼                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚  â”‚          Document Loaders               â”‚                 â”‚
    â”‚  â”‚  (TextLoader, PDFLoader, CSVLoader)     â”‚                 â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚                   â”‚                                          â”‚
    â”‚                   â–¼                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚  â”‚      Document Objects (with metadata)   â”‚                 â”‚
    â”‚  â”‚  [{content: "...", metadata: {...}}]    â”‚                 â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚                   â”‚                                          â”‚
    â”‚                   â–¼                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚  â”‚    RecursiveCharacterTextSplitter       â”‚                 â”‚
    â”‚  â”‚    (chunk_size=200, overlap=50)         â”‚                 â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚                   â”‚                                          â”‚
    â”‚                   â–¼                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚  â”‚     Chunks (preserves metadata)         â”‚                 â”‚
    â”‚  â”‚  [{content: "chunk1", metadata: {...}}] â”‚                 â”‚
    â”‚  â”‚  [{content: "chunk2", metadata: {...}}] â”‚                 â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚                   â”‚                                          â”‚
    â”‚                   â–¼                                          â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
    â”‚  â”‚         Embed & Store                   â”‚                 â”‚
    â”‚  â”‚        (VectorDB ready)                 â”‚                 â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
    â”‚                                                              â”‚
    â”‚  Key Points:                                                 â”‚
    â”‚  â€¢ Different loaders for different file types                â”‚
    â”‚  â€¢ Metadata preserved through pipeline                       â”‚
    â”‚  â€¢ Splitting maintains source attribution                    â”‚
    â”‚  â€¢ Ready for vectorDB ingestion                              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Document Loading from Files")

    # create temporary files
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)

        # create sample text files
        file1 = temp_path / "ai_overview.txt"
        file1.write_text(
            cleandoc(
                """
            Artificial Intelligence (AI) Overview

            AI is the simulation of human intelligence by machines. It encompasses various
            subfields including machine learning, natural language processing, computer
            vision, and robotics. Modern AI systems can perform complex tasks like image
            recognition, language translation, and decision-making.
            """
            )
        )

        file2 = temp_path / "ml_intro.txt"
        file2.write_text(
            cleandoc(
                """
            Machine Learning Introduction

            Machine learning is a method of data analysis that automates analytical model
            building. It uses algorithms that iteratively learn from data, allowing computers
            to find hidden insights without being explicitly programmed where to look.
            """
            )
        )

        print(f"ğŸ“ created temporary directory: {temp_dir}")
        print(f"   file 1: {file1.name}")
        print(f"   file 2: {file2.name}")

        # load documents
        print("\nğŸ“„ loading documents...")
        loader1 = TextLoader(str(file1))
        loader2 = TextLoader(str(file2))

        docs1 = loader1.load()
        docs2 = loader2.load()

        all_docs = docs1 + docs2

        print(f"   loaded {len(all_docs)} documents")
        for i, doc in enumerate(all_docs, 1):
            source = Path(doc.metadata["source"]).name
            print(f"   {i}. {source}: {len(doc.page_content)} characters")

        # split documents
        print("\nâœ‚ï¸ splitting documents into chunks...")
        splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50)
        chunks = splitter.split_documents(all_docs)
        print(f"   created {len(chunks)} chunks from {len(all_docs)} documents")


# endregion


# region Demo 6: RAG with Custom Retriever
@requires_openai
def demo_custom_retriever() -> None:
    """
    demonstrate RAG with custom retriever configuration

    Custom Retriever Configuration:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           Retriever Configuration Options                    â”‚
    â”‚                                                              â”‚
    â”‚  Configuration 1: Top-K Retrieval                            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
    â”‚  â”‚  retriever = vectorstore.      â”‚                          â”‚
    â”‚  â”‚    as_retriever(               â”‚                          â”‚
    â”‚  â”‚      search_kwargs={"k": 3}    â”‚                          â”‚
    â”‚  â”‚    )                           â”‚                          â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
    â”‚               â”‚                                              â”‚
    â”‚               â–¼                                              â”‚
    â”‚  Query â†’ VectorDB â†’ Top 3 docs (by similarity)               â”‚
    â”‚                                                              â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
    â”‚                                                              â”‚
    â”‚  Configuration 2: MMR (Diversity)                            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
    â”‚  â”‚  retriever = vectorstore.      â”‚                          â”‚
    â”‚  â”‚    as_retriever(               â”‚                          â”‚
    â”‚  â”‚      search_type="mmr",        â”‚                          â”‚
    â”‚  â”‚      search_kwargs={           â”‚                          â”‚
    â”‚  â”‚        "k": 2,                 â”‚                          â”‚
    â”‚  â”‚        "fetch_k": 4            â”‚                          â”‚
    â”‚  â”‚      }                         â”‚                          â”‚
    â”‚  â”‚    )                           â”‚                          â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
    â”‚               â”‚                                              â”‚
    â”‚               â–¼                                              â”‚
    â”‚  Query â†’ VectorDB â†’ Fetch 4 â†’ Diversity Rerank â†’ Top 2       â”‚
    â”‚                                                              â”‚
    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
    â”‚                                                              â”‚
    â”‚  Configuration 3: Score Threshold                            â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
    â”‚  â”‚  retriever = vectorstore.      â”‚                          â”‚
    â”‚  â”‚    as_retriever(               â”‚                          â”‚
    â”‚  â”‚      search_type=              â”‚                          â”‚
    â”‚  â”‚        "similarity_score_      â”‚                          â”‚
    â”‚  â”‚         threshold",            â”‚                          â”‚
    â”‚  â”‚      search_kwargs={           â”‚                          â”‚
    â”‚  â”‚        "score_threshold": 0.3, â”‚                          â”‚
    â”‚  â”‚        "k": 3                  â”‚                          â”‚
    â”‚  â”‚      }                         â”‚                          â”‚
    â”‚  â”‚    )                           â”‚                          â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
    â”‚               â”‚                                              â”‚
    â”‚               â–¼                                              â”‚
    â”‚  Query â†’ VectorDB â†’ Filter by score > 0.3 â†’ Return matches   â”‚
    â”‚                                                              â”‚
    â”‚  Key Parameters:                                             â”‚
    â”‚  â€¢ k: How many docs to retrieve                              â”‚
    â”‚  â€¢ search_type: similarity | mmr | similarity_score_thresholdâ”‚
    â”‚  â€¢ fetch_k: Candidates for MMR reranking                     â”‚
    â”‚  â€¢ score_threshold: Minimum similarity score                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("Custom Retriever Configuration")

    # load persistent vector store
    vectorstore = get_or_create_vectorstore()

    # configuration 1: retrieve more documents
    print("âš™ï¸ configuration 1: retrieve top 3 documents")
    retriever1 = vectorstore.as_retriever(search_kwargs={"k": 3})

    query = "Explain AI and machine learning"

    docs = retriever1.invoke(query)
    print(f"   retrieved {len(docs)} documents")

    # configuration 2: use MMR for diversity
    print("\nâš™ï¸ configuration 2: MMR retrieval (diversity)")
    retriever2 = vectorstore.as_retriever(
        search_type="mmr", search_kwargs={"k": 2, "fetch_k": 4}
    )

    docs = retriever2.invoke(query)
    print(f"   retrieved {len(docs)} diverse documents")

    # configuration 3: similarity threshold
    print("\nâš™ï¸ configuration 3: similarity score threshold")
    retriever3 = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"score_threshold": 0.3, "k": 3},
    )

    docs = retriever3.invoke(query)
    print(f"   retrieved {len(docs)} documents above threshold")

    print("\nğŸ’¡ retriever configurations:")
    print("   â€¢ k: number of documents to retrieve")
    print("   â€¢ search_type: similarity, mmr, or similarity_score_threshold")
    print("   â€¢ fetch_k: for MMR, candidates to fetch before reranking")
    print("   â€¢ score_threshold: minimum similarity score")


# endregion


# region Demo 7: Multi-Query RAG
@requires_openai
def demo_multi_query() -> None:
    """
    demonstrate generating multiple query perspectives for improved retrieval

    Multi-Query RAG Flow:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                     Multi-Query RAG Pipeline                    â”‚
    â”‚                                                                 â”‚
    â”‚  Original Query                                                 â”‚
    â”‚       â”‚                                                         â”‚
    â”‚       â–¼                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
    â”‚  â”‚   LLM   â”‚ (generate variations)                              â”‚
    â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                                    â”‚
    â”‚       â”‚                                                         â”‚
    â”‚       â–¼                                                         â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
    â”‚  â”‚  Q1  â”‚  Q2  â”‚  Q3  â”‚  Q4   â”‚  (variations)                   â”‚
    â”‚  â””â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”¬â”€â”€â”€â”€â”˜                                 â”‚
    â”‚     â”‚      â”‚      â”‚      â”‚                                      â”‚
    â”‚     â–¼      â–¼      â–¼      â–¼   (parallel search)                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
    â”‚  â”‚      VectorDB Search     â”‚                                   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
    â”‚               â”‚                                                 â”‚
    â”‚               â–¼                                                 â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
    â”‚  â”‚  Deduplicate Docs   â”‚                                        â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
    â”‚             â”‚                                                   â”‚
    â”‚             â–¼                                                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
    â”‚  â”‚  Combine Context    â”‚                                        â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
    â”‚             â”‚                                                   â”‚
    â”‚             â–¼                                                   â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
    â”‚  â”‚   LLM Generation    â”‚ (answer using context)                 â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
    â”‚             â”‚                                                   â”‚
    â”‚             â–¼                                                   â”‚
    â”‚        Final Answer                                             â”‚
    â”‚                                                                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Key improvement: generates multiple perspectives of the query to improve
    retrieval recall and capture different aspects of the question.
    """
    print_section("Multi-Query RAG")

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)

    # load persistent vector store
    vectorstore = get_or_create_vectorstore()

    original_query = "What is AI?"

    # generate query variations using LLM
    print(f"ğŸ” original query: '{original_query}'")
    print("\nğŸ”„ generating query variations...")

    query_prompt = ChatPromptTemplate.from_template(
        cleandoc(
            """
        you are an AI assistant that generates alternative search queries.
        given the original query, generate 3 different versions that capture the
        same intent but use different wording.

        original query: {question}

        alternative queries (one per line):
        """
        )
    )

    query_chain = query_prompt | llm | StrOutputParser()
    alternatives = query_chain.invoke({"question": original_query})

    queries = [original_query] + [q.strip("123. -") for q in alternatives.split("\n") if q.strip()]

    print("\nğŸ“‹ query variations:")
    for i, q in enumerate(queries[:4], 1):
        print(f"   {i}. {q}")

    # retrieve documents for each query
    print("\nğŸ“š retrieving documents for each variation...")
    all_docs = []
    seen_contents = set()

    for query in queries[:4]:
        docs = vectorstore.similarity_search(query, k=2)
        for doc in docs:
            content = doc.page_content[:100]
            if content not in seen_contents:
                all_docs.append(doc)
                seen_contents.add(content)

    print(f"   retrieved {len(all_docs)} unique documents (deduped)")

    print("\nğŸ’¡ multi-query benefits:")
    print("   â€¢ captures different aspects of the question")
    print("   â€¢ improves recall (finds more relevant docs)")
    print("   â€¢ handles ambiguous or underspecified queries")

    # next step: generate answer using retrieved documents
    print("\nğŸ¤– generating answer using retrieved documents...")

    # combine documents into context
    context = "\n\n".join([doc.page_content for doc in all_docs])

    # create RAG prompt
    rag_prompt = ChatPromptTemplate.from_template(
        cleandoc(
            """
        answer the question based only on the following context:

        {context}

        question: {question}

        answer:
        """
        )
    )

    # generate answer
    answer_chain = rag_prompt | llm | StrOutputParser()
    answer = answer_chain.invoke({"context": context, "question": original_query})

    print(f"\nğŸ“ final answer:")
    print(f"   {answer[:300]}{'...' if len(answer) > 300 else ''}")

    print("\nğŸ¯ next steps after retrieval:")
    print("   1. combine retrieved docs into context")
    print("   2. create RAG prompt (context + question)")
    print("   3. generate answer using LLM")
    print("   4. (optional) add citations/source tracking")
    print("   5. (optional) implement streaming for long answers")


# endregion


# region Demo 8: RAG Chain Comparison
@requires_openai
def demo_chain_comparison() -> None:
    """
    compare different RAG chain approaches

    RAG Chain Types:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                RAG Chain Architecture Patterns               â”‚
    â”‚                                                              â”‚
    â”‚  1. STUFF Chain (recommended for <4 docs):                   â”‚
    â”‚     Query â†’ Retrieve â†’ [All Docs] â†’ LLM â†’ Answer             â”‚
    â”‚                                                              â”‚
    â”‚  2. MAP_REDUCE Chain (large doc sets):                       â”‚
    â”‚     Query â†’ Retrieve â†’ [Doc1] â†’ LLM â†’ Summary1               â”‚
    â”‚                     â”œâ”€ [Doc2] â†’ LLM â†’ Summary2               â”‚
    â”‚                     â””â”€ [Doc3] â†’ LLM â†’ Summary3               â”‚
    â”‚                           â†“                                  â”‚
    â”‚                   [All Summaries] â†’ LLM â†’ Answer             â”‚
    â”‚                                                              â”‚
    â”‚  3. REFINE Chain (iterative improvement):                    â”‚
    â”‚     Query â†’ Retrieve â†’ [Doc1] â†’ LLM â†’ Draft1                 â”‚
    â”‚                     â”œâ”€ [Draft1 + Doc2] â†’ LLM â†’ Draft2        â”‚
    â”‚                     â””â”€ [Draft2 + Doc3] â†’ LLM â†’ Final         â”‚
    â”‚                                                              â”‚
    â”‚  4. MAP_RERANK Chain (score-based selection):                â”‚
    â”‚     Query â†’ Retrieve â†’ [Doc1] â†’ LLM â†’ (Answer1, Score1)      â”‚
    â”‚                     â”œâ”€ [Doc2] â†’ LLM â†’ (Answer2, Score2)      â”‚
    â”‚                     â””â”€ [Doc3] â†’ LLM â†’ (Answer3, Score3)      â”‚
    â”‚                           â†“                                  â”‚
    â”‚                   Select Highest Score â†’ Best Answer         â”‚
    â”‚                                                              â”‚
    â”‚  Trade-offs:                                                 â”‚
    â”‚  â€¢ Stuff: Fast, cheap, but limited by context window         â”‚
    â”‚  â€¢ Map_Reduce: Handles many docs, but more expensive         â”‚
    â”‚  â€¢ Refine: Good for iterative improvement, sequential        â”‚
    â”‚  â€¢ Map_Rerank: Best quality, but highest cost                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print_section("RAG Chain Comparison")

    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

    # load persistent vector store
    vectorstore = get_or_create_vectorstore()
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    query = "What are the key concepts in AI?"

    # approach 1: basic stuff chain (most common)
    print("ğŸ“Š approach 1: stuff chain (recommended)")
    print("   puts all docs in single prompt")

    def format_docs(docs: list[Document]) -> str:
        return "\n\n".join(doc.page_content for doc in docs)

    prompt_template = cleandoc(
        """
        answer based on the following context:

        {context}

        question: {question}

        answer:
        """
    )
    prompt = ChatPromptTemplate.from_template(prompt_template)

    chain1 = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    response1 = chain1.invoke(query)
    print(f"   answer: {response1[:100]}...")

    # approach 2: with source citations
    print("\nğŸ“Š approach 2: with source citations")
    print("   includes metadata in response")

    def format_docs_with_sources(docs: list[Document]) -> str:
        formatted = []
        for i, doc in enumerate(docs, 1):
            source = doc.metadata.get("source", "unknown")
            formatted.append(f"[{i}] source: {source}\n{doc.page_content}")
        return "\n\n".join(formatted)

    prompt_with_sources = ChatPromptTemplate.from_template(
        cleandoc(
            """
        answer based on the following sources:

        {context}

        question: {question}

        answer (cite sources using [1], [2] etc.):
        """
        )
    )

    chain2 = (
        {"context": retriever | format_docs_with_sources, "question": RunnablePassthrough()}
        | prompt_with_sources
        | llm
        | StrOutputParser()
    )

    response2 = chain2.invoke(query)
    print(f"   answer: {response2[:100]}...")

    print("\nğŸ“‹ chain type comparison:")
    print("   stuff: best for small doc sets (< 4 docs)")
    print("   map_reduce: for large doc sets, parallel processing")
    print("   refine: iterative refinement across docs")
    print("   map_rerank: scores each doc, picks best")


# endregion


# region Main Execution
def show_menu(has_openai: bool) -> None:
    """display interactive demo menu"""
    print("\n" + "=" * 70)
    print("  RAG (Retrieval-Augmented Generation) - Practical Examples")
    print("=" * 70)
    print("\nğŸ“š Available Demos:\n")

    demos = [
        ("1", "Basic RAG Pipeline", "complete RAG flow with ChromaDB and LCEL", True),
        ("2", "Text Chunking Strategies", "compare different chunking approaches", False),
        ("3", "Similarity Search Methods", "basic search, scores, and MMR", True),
        ("4", "Metadata Filtering", "filter documents by metadata during search", True),
        ("5", "Document Loading from Files", "load and process text files", False),
        ("6", "Custom Retriever Configuration", "configure retriever parameters", True),
        ("7", "Multi-Query RAG", "generate query variations for better recall", True),
        ("8", "RAG Chain Comparison", "compare different chain approaches", True),
    ]

    for num, name, desc, needs_api in demos:
        api_marker = "ğŸ”‘" if needs_api else "  "
        status = "" if (has_openai or not needs_api) else " âš ï¸ (needs API key)"
        print(f"  {api_marker} [{num}] {name}")
        print(f"      {desc}{status}")
        print()

    print("  [a] Run all demos")
    print("  [q] Quit")
    print("\n" + "=" * 70)
    if not has_openai:
        print("  âš ï¸  Some demos require OPENAI_API_KEY (marked with ğŸ”‘)")
        print("=" * 70)


def run_selected_demos(selections: str, has_openai: bool) -> bool:
    """
    run selected demos based on user input

    Args:
        selections: user input string (numbers, 'a', or 'q')
        has_openai: whether OpenAI API key is available

    Returns:
        True to continue, False to quit
    """
    selections = selections.strip().lower()

    if selections == 'q':
        return False

    # map demo numbers to functions
    demo_map = {
        '1': ('Basic RAG Pipeline', demo_basic_rag, True),
        '2': ('Text Chunking Strategies', demo_text_chunking, False),
        '3': ('Similarity Search Methods', demo_similarity_search, True),
        '4': ('Metadata Filtering', demo_metadata_filtering, True),
        '5': ('Document Loading from Files', demo_document_loading, False),
        '6': ('Custom Retriever Configuration', demo_custom_retriever, True),
        '7': ('Multi-Query RAG', demo_multi_query, True),
        '8': ('RAG Chain Comparison', demo_chain_comparison, True),
    }

    # determine which demos to run
    if selections == 'a':
        demos_to_run = list(demo_map.keys())
    else:
        # parse comma-separated or space-separated numbers
        demos_to_run = [s.strip() for s in selections.replace(',', ' ').split() if s.strip() in demo_map]

    if not demos_to_run:
        print("\nâš ï¸  invalid selection. please enter demo numbers (1-8), 'a' for all, or 'q' to quit")
        return True

    # run selected demos
    print("\n" + "=" * 70)
    print(f"  Running {len(demos_to_run)} demo(s)")
    print("=" * 70)

    for demo_num in demos_to_run:
        name, func, needs_api = demo_map[demo_num]

        if needs_api and not has_openai:
            print(f"\nâš ï¸  skipping demo {demo_num}: {name} (requires OPENAI_API_KEY)")
            continue

        try:
            func()
        except KeyboardInterrupt:
            print("\n\nâš ï¸  demo interrupted by user")
            return False
        except Exception as e:
            print(f"\nâŒ error in demo {demo_num}: {e}")
            continue

    print("\n" + "=" * 70)
    print("  âœ… selected demos complete!")
    print("=" * 70)
    print("\nğŸ’¡ key takeaways:")
    print("  1. ChromaDB provides easy local vector storage")
    print("  2. RecursiveCharacterTextSplitter is best for most use cases")
    print("  3. MMR balances relevance with diversity")
    print("  4. Metadata filtering narrows search scope efficiently")
    print("  5. Multi-query improves recall for ambiguous questions")
    print("  6. LCEL chains are composable and production-ready")
    print("\nğŸ“š explore:")
    print("  â€¢ different embedding models (text-embedding-3-large)")
    print("  â€¢ persistent ChromaDB storage")
    print("  â€¢ other vector stores (FAISS, Pinecone)")
    print("  â€¢ advanced retrieval (parent document, self-query)")

    return True


def main() -> None:
    """run practical demonstrations with interactive menu"""
    has_openai, _ = check_api_keys()

    if not has_openai:
        print("\nâš ï¸  OPENAI_API_KEY not found")
        print("   some demos will be skipped (marked with ğŸ”‘)")
        print("   set it with: export OPENAI_API_KEY='your-key-here'")

    # interactive menu loop
    while True:
        show_menu(has_openai)

        try:
            selection = input("\nğŸ¯ select demo(s) (e.g., '1', '1,3,5', or 'a' for all): ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n\nğŸ‘‹ exiting...")
            break

        if not run_selected_demos(selection, has_openai):
            print("\nğŸ‘‹ exiting...")
            break

        # pause before showing menu again
        try:
            input("\nâ¸ï¸  Press Enter to continue...")
        except (EOFError, KeyboardInterrupt):
            print("\n\nğŸ‘‹ exiting...")
            break


if __name__ == "__main__":
    main()
# endregion
