# Module 1: LangChain Basics

**Purpose**: Learn LangChain fundamentals by seeing how it simplifies your Phase 3/4 implementations

---

## Learning Objectives

By the end of this module, you will:
- Understand LangChain's core abstractions (chains, prompts, memory)
- Convert your Phase 3 code to LangChain equivalents
- Know when LangChain helps vs adds overhead
- Use LangChain for real-world agent tasks

---

## Topics Covered

### 1. Prompts & Templates
**What you built**: String formatting with f-strings
**LangChain**: `PromptTemplate`, `ChatPromptTemplate`

```python
# Your way (Phase 2)
prompt = f"You are a {role}. {task}"

# LangChain way
from langchain.prompts import ChatPromptTemplate
template = ChatPromptTemplate.from_messages([
    ("system", "You are a {role}"),
    ("user", "{task}")
])
prompt = template.format_messages(role="assistant", task="help me")
```

### 2. LLM Integration
**What you built**: Raw OpenAI/Anthropic API calls
**LangChain**: `ChatOpenAI`, `ChatAnthropic` with unified interface

```python
# Your way (Phase 2)
from openai import OpenAI
client = OpenAI()
response = client.chat.completions.create(model="gpt-4o", messages=[...])

# LangChain way
from langchain_openai import ChatOpenAI
llm = ChatOpenAI(model="gpt-4o")
response = llm.invoke("Your prompt here")
```

### 3. Chains
**What you built**: Manual function composition
**LangChain**: `LLMChain`, `SequentialChain`, `LCEL`

```python
# Your way
def pipeline(text):
    summary = summarize(text)
    sentiment = analyze_sentiment(summary)
    return sentiment

# LangChain way (LCEL)
from langchain_core.runnables import RunnablePassthrough
chain = summarize_chain | sentiment_chain
result = chain.invoke({"text": text})
```

### 4. Memory
**What you built**: `ChatMemory` class (Phase 3)
**LangChain**: `RunnableWithMessageHistory` (modern LCEL approach)

```python
# Your way (Phase 3)
memory = ChatMemory(strategy="sliding_window", max_messages=10)
memory.add_message("user", "Hello")

# LangChain way (modern LangChain 1.0+ with LCEL)
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory

# simple chain
chain = prompt | llm

# add message history
store = {}  # session_id -> chat history

def get_session_history(session_id: str):
    if session_id not in store:
        store[session_id] = InMemoryChatMessageHistory()
    return store[session_id]

chain_with_history = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

# use with session ID
chain_with_history.invoke({"input": "Hello"}, config={"configurable": {"session_id": "user123"}})
```

> **Note**: For legacy `ConversationBufferMemory` pattern, see [04_memory](04_memory/README.md#legacy-patterns--deprecated)

### 5. RAG
**What you built**: Full RAG pipeline (Phase 3)
**LangChain**: LCEL-based RAG chains (modern approach)

```python
# Your way (Phase 3)
chunks = chunker.chunk(document)
embeddings = embedder.embed(chunks)
db.add(chunks, embeddings)
results = db.search(query)
context = assemble_context(results)
response = llm.generate(prompt + context)

# LangChain way (modern LangChain 1.0+ with LCEL)
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# setup vectorstore
vectorstore = Chroma.from_documents(documents, embeddings)
retriever = vectorstore.as_retriever()

# create RAG chain with LCEL
template = ChatPromptTemplate.from_template("""
Answer based on context:
{context}

Question: {question}
""")

rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | template
    | llm
)

response = rag_chain.invoke("your question")
```

> **Note**: For legacy `RetrievalQA` pattern, see [05_rag](05_rag/README.md#legacy-patterns--deprecated)

### 6. Agents & Tools
**What you built**: `ReActAgent`, `ToolRegistry` (Phase 4)
**LangChain**: `create_agent` (LangGraph), `@tool` decorator

```python
# Your way (Phase 4)
class WebSearchTool(BaseTool):
    def execute(self, query: str) -> ToolResult:
        results = search_api(query)
        return ToolResult.ok(results)

registry = ToolRegistry()
registry.register(WebSearchTool())
agent = ReActAgent(registry=registry)

# LangChain way (modern LangChain 1.0+)
from langchain.agents import create_agent
from langchain.tools import tool
from langchain_core.messages import HumanMessage

@tool
def web_search(query: str) -> str:
    """search the web for information"""
    return search_api(query)

# create_agent returns executable CompiledStateGraph
agent = create_agent(model=llm, tools=[web_search])

# execute with message-based API
result = agent.invoke({"messages": [HumanMessage(content="search for LangChain")]})
```

> **Note**: For legacy `create_react_agent` + `AgentExecutor` pattern, see [06_agents_tools](06_agents_tools/README.md#legacy-patterns--deprecated)

---

## Module Structure

```
01_langchain_basics/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ migration_examples.py        # âœ… Side-by-side: Your code â†’ LangChain (6 comparisons)
â”œâ”€â”€ langchain_concepts_demo.py   # âœ… Conceptual overview with comparisons
â”œâ”€â”€ langchain_rag_chatbot.py     # âœ… Step-by-step RAG chatbot walkthrough
â”œâ”€â”€ 01_prompts/                  # âœ… PromptTemplate, ChatPromptTemplate, FewShotPromptTemplate
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ concepts.py              # Conceptual (no API key)
â”‚   â””â”€â”€ practical.py             # Hands-on (requires API key)
â”œâ”€â”€ 02_llm_integration/          # âœ… ChatOpenAI, ChatAnthropic, unified interface
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ concepts.py              # Conceptual (no API key)
â”‚   â””â”€â”€ practical.py             # Hands-on (requires API keys)
â”œâ”€â”€ 03_chains/                   # âœ… LLMChain, SequentialChain, LCEL syntax
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ concepts.py              # Conceptual (no API key)
â”‚   â””â”€â”€ practical.py             # Hands-on (requires API keys)
â”œâ”€â”€ 04_memory/                   # âœ… ConversationBufferMemory, ConversationSummaryMemory
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ concepts.py              # Conceptual (no API key)
â”‚   â””â”€â”€ practical.py             # Hands-on (requires API key)
â”œâ”€â”€ 05_rag/                      # âœ… RetrievalQA, vector stores, document loaders
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ concepts.py              # Conceptual (no API key)
â”‚   â””â”€â”€ practical.py             # Hands-on (requires API key)
â””â”€â”€ 06_agents_tools/             # âœ… create_react_agent, @tool, AgentExecutor
    â”œâ”€â”€ README.md
    â”œâ”€â”€ concepts.py              # Conceptual (no API key)
    â””â”€â”€ practical.py             # Hands-on (requires API key)
```

**Current Status**: All 6 modules complete! (Prompts, LLM integration, chains, memory, RAG, agents & tools)

---

## Key Concepts

### LangChain Abstraction Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Your Application Logic                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Chains/Agents (High-level orchestration)           â”‚
â”‚    - RetrievalQA, ConversationalRetrievalChain      â”‚
â”‚    - create_react_agent, AgentExecutor              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Components (Mid-level building blocks)             â”‚
â”‚    - Prompts, Memory, Callbacks                     â”‚
â”‚    - Vector Stores, Document Loaders                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  LLMs (Low-level provider integration)              â”‚
â”‚    - ChatOpenAI, ChatAnthropic, HuggingFace         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         Provider APIs (OpenAI, Anthropic, etc.)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### LCEL (LangChain Expression Language)

**Philosophy**: Chain components with `|` operator

```python
# Traditional
chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(input)

# LCEL (modern LangChain)
chain = prompt | llm | output_parser
result = chain.invoke(input)
```

**Benefits**:
- More readable (Unix pipe style)
- Streaming support built-in
- Async by default
- Better error handling

---

## Prerequisites

Before starting this module:
- âœ… Completed Phase 2 (LLM fundamentals)
- âœ… Completed Phase 3 (RAG, chat, function calling)
- âœ… Completed Phase 4 (agents)

---

## Installation

âœ… **Already Installed!** All LangChain packages are now available:

```bash
# Installed packages:
# - langchain==1.0.5
# - langchain-openai==1.0.2
# - langchain-anthropic==1.0.3
# - langchain-chroma==1.0.0
# - langchain-community==0.4.1
# - langchain-core==1.0.4
```

To install in a new environment:
```bash
uv add langchain langchain-openai langchain-anthropic langchain-chroma langchain-community
```

---

## Running Examples

```bash
# âœ… Conceptual Foundation (no API key needed):
uv run python -m phase7_frameworks.01_langchain_basics.migration_examples
uv run python -m phase7_frameworks.01_langchain_basics.langchain_concepts_demo
uv run python -m phase7_frameworks.01_langchain_basics.langchain_rag_chatbot

# âœ… Prompts Module:
uv run python -m phase7_frameworks.01_langchain_basics.01_prompts.concepts
uv run python -m phase7_frameworks.01_langchain_basics.01_prompts.practical

# âœ… LLM Integration Module:
uv run python -m phase7_frameworks.01_langchain_basics.02_llm_integration.concepts
uv run python -m phase7_frameworks.01_langchain_basics.02_llm_integration.practical

# âœ… Chains Module:
uv run python -m phase7_frameworks.01_langchain_basics.03_chains.concepts
uv run python -m phase7_frameworks.01_langchain_basics.03_chains.practical

# âœ… Memory Module:
uv run python -m phase7_frameworks.01_langchain_basics.04_memory.concepts
uv run python -m phase7_frameworks.01_langchain_basics.04_memory.practical

# âœ… RAG Module:
uv run python -m phase7_frameworks.01_langchain_basics.05_rag.concepts
uv run python -m phase7_frameworks.01_langchain_basics.05_rag.practical

# âœ… Agents & Tools Module:
uv run python -m phase7_frameworks.01_langchain_basics.06_agents_tools.concepts
uv run python -m phase7_frameworks.01_langchain_basics.06_agents_tools.practical
```

**Organization**: Each module has:
- `concepts.py` - Learn patterns without API key
- `practical.py` - Practice with real LLM calls (requires `OPENAI_API_KEY` in `.env`)

**âœ¨ New Feature - Visual Documentation**: All 49 demos across 6 modules now include comprehensive ASCII diagrams showing:
- ğŸ“Š Architecture and workflow visualization
- â†’ Step-by-step data flow with arrows
- âœ… Benefits highlighted for each pattern
- âš ï¸  Important limitations and cautions
- ğŸ’¡ Implementation details and code patterns
- ğŸ¯ Real-world use cases

Example from Memory module:
```
Buffer Memory Pattern:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Buffer Memory: Full Conversation History Storage      â”‚
â”‚                                                             â”‚
â”‚  Turn 1:                                                    â”‚
â”‚     User: "Hi, I'm learning about LangChain memory"         â”‚
â”‚           â–¼                                                 â”‚
â”‚     Memory: [] (empty) â†’ Store message                      â”‚
â”‚           â–¼                                                 â”‚
â”‚     LLM: "Great! LangChain memory helps..."                 â”‚
â”‚                                                             â”‚
â”‚  âœ… Benefit: Perfect recall (all context retained)          â”‚
â”‚  âš ï¸  Caution: Unbounded token growth over time              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Exercises

### Exercise 1: Prompt Templates
Convert your Phase 2 prompt engineering patterns to LangChain templates.

### Exercise 2: RAG Migration
Refactor your Phase 3 RAG pipeline using `RetrievalQA`.

### Exercise 3: Agent with Tools
Rebuild your Phase 4 `ReActAgent` using `create_react_agent`.

### Exercise 4: Memory Strategies
Compare `ConversationBufferMemory` vs your custom `ChatMemory`.

Solutions in `solutions/` directory.

---

## When to Use LangChain

### âœ… Good Use Cases
- Building agents with multiple tools
- RAG with standard patterns
- Need provider flexibility (OpenAI â†” Anthropic)
- Team standardization
- Want LangSmith monitoring

### âŒ Skip LangChain
- Simple single LLM call
- Maximum performance critical
- Custom logic doesn't fit patterns
- Framework overhead too high

---

## Common Pitfalls

### 1. Over-Abstraction
```python
# Bad: Using framework for simple task
chain = prompt | llm | output_parser
result = chain.invoke({"input": "Hello"})

# Good: Direct API call
response = llm.invoke("Hello")
```

### 2. Version Lock-in
```python
# Bad: Unversioned imports
from langchain.chains import RetrievalQA  # might break

# Good: Explicit versions in requirements.txt
langchain==0.1.0
langchain-openai==0.0.5
```

### 3. Hidden Costs
```python
# Watch out: LangChain can make many API calls
chain = RetrievalQA.from_chain_type(llm, retriever)
result = chain.invoke(query)  # How many LLM calls? Check docs!
```

---

## Next Steps

After completing this module:
1. Build a small RAG chatbot with LangChain
2. Compare implementation complexity vs your Phase 3 code
3. Move to Module 2 (LangGraph) for multi-agent workflows
4. Decide which patterns to adopt in production

---

## Resources

- [LangChain Docs](https://python.langchain.com/)
- [LCEL Guide](https://python.langchain.com/docs/expression_language/)
- [Agent Types](https://python.langchain.com/docs/modules/agents/agent_types/)
- [LangSmith Monitoring](https://docs.smith.langchain.com/)
