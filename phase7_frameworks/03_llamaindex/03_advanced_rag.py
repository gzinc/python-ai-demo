"""
LlamaIndex Advanced RAG - Production Patterns

This module demonstrates advanced RAG techniques:
- Reranking for better retrieval
- Query transformations
- Response synthesis modes
- Hybrid search (keyword + vector)
- Custom retrievers

Run with: uv run python phase7_frameworks/03_llamaindex/03_advanced_rag.py
"""

from inspect import cleandoc

from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

from common.demo_menu import Demo, MenuRunner
from common.util.utils import check_api_keys, print_section


# region Demo 1: Custom Retrieval with Top-K


def demo_custom_retrieval():
    """demonstrate controlling retrieval parameters"""
    print_section("Demo 1: Custom Retrieval Parameters")

    documents = [
        Document(text="LlamaIndex is a data framework for LLM applications.", metadata={"id": 1}),
        Document(text="Vector databases store embeddings for semantic search.", metadata={"id": 2}),
        Document(text="RAG combines retrieval with generation for accurate responses.", metadata={"id": 3}),
        Document(text="Query engines handle the end-to-end RAG pipeline.", metadata={"id": 4}),
        Document(text="Chat engines add conversational memory to RAG systems.", metadata={"id": 5}),
    ]

    index = VectorStoreIndex.from_documents(documents)
    query = "What is RAG?"

    # test different top_k values
    top_k_values = [1, 2, 5]

    print(f"ğŸ’¬ Query: {query}\n")

    for top_k in top_k_values:
        retriever = VectorIndexRetriever(index=index, similarity_top_k=top_k)
        query_engine = RetrieverQueryEngine(retriever=retriever)

        response = query_engine.query(query)

        print(f"ğŸ” Top-K = {top_k}")
        print(f"   Retrieved {len(response.source_nodes)} chunks")
        print(f"   Response: {response.response[:100]}...")
        print()

    print(
        cleandoc("""
        ğŸ’¡ Top-K Trade-offs:
        â€¢ Lower K (1-2): Faster, focused but may miss context
        â€¢ Medium K (3-5): Balanced approach (most common)
        â€¢ Higher K (10+): More context but slower, more noise
        â€¢ Optimal K depends on chunk size and query complexity
    """)
    )


# endregion


# region Demo 2: Query Transformation


def demo_query_transformation():
    """demonstrate query rewriting for better retrieval"""
    print_section("Demo 2: Query Transformation")

    documents = [
        Document(
            text=cleandoc("""
                Query transformation improves retrieval by reformulating user questions.
                Techniques include: decomposition, multi-query, and HyDE (Hypothetical Document Embeddings).
                Decomposition breaks complex queries into simpler sub-queries.
                Multi-query generates variations to capture different aspects.
            """)
        ),
        Document(
            text=cleandoc("""
                HyDE (Hypothetical Document Embeddings) generates a hypothetical answer first.
                The hypothetical answer is embedded and used for retrieval.
                This often finds better matches than searching with the question directly.
                Works well when questions and answers have different vocabulary.
            """)
        ),
    ]

    index = VectorStoreIndex.from_documents(documents)

    print("ğŸ’¬ Original Query: 'query techniques'\n")

    # standard retrieval
    query_engine = index.as_query_engine(similarity_top_k=1)
    response = query_engine.query("query techniques")

    print("ğŸ“ Standard Retrieval:")
    print(f"   {response.response[:150]}...\n")

    # with query understanding (built into LlamaIndex)
    print("ğŸ’¡ Query Transformation Concepts:")
    print(
        cleandoc("""
        â€¢ Decomposition: "What is X and how does Y work?"
          â†’ Query 1: "What is X?"
          â†’ Query 2: "How does Y work?"

        â€¢ Multi-query: "best practices for RAG"
          â†’ "RAG best practices"
          â†’ "RAG optimization techniques"
          â†’ "effective RAG strategies"

        â€¢ HyDE: "How do I improve retrieval?"
          â†’ Generate: "To improve retrieval, use query transformation..."
          â†’ Search with generated answer (better semantic match)
    """)
    )


# endregion


# region Demo 3: Response Synthesis Strategies


def demo_synthesis_strategies():
    """demonstrate different ways to synthesize final responses"""
    print_section("Demo 3: Response Synthesis Strategies")

    documents = [
        Document(text="Step 1: Ingest your data from various sources.", metadata={"step": 1}),
        Document(text="Step 2: Parse and chunk documents into smaller pieces.", metadata={"step": 2}),
        Document(text="Step 3: Create embeddings for each chunk.", metadata={"step": 3}),
        Document(text="Step 4: Store embeddings in a vector database.", metadata={"step": 4}),
        Document(text="Step 5: Retrieve relevant chunks based on query.", metadata={"step": 5}),
        Document(text="Step 6: Synthesize final response using LLM.", metadata={"step": 6}),
    ]

    index = VectorStoreIndex.from_documents(documents)
    query = "What are the steps in a RAG pipeline?"

    synthesis_modes = {
        "compact": "Combine chunks efficiently before LLM call (default)",
        "refine": "Iteratively refine answer with each chunk",
        "tree_summarize": "Build hierarchical summary tree",
    }

    print(f"ğŸ’¬ Query: {query}\n")

    for mode, description in synthesis_modes.items():
        query_engine = index.as_query_engine(
            response_mode=mode, similarity_top_k=6  # retrieve all steps
        )

        response = query_engine.query(query)

        print(f"ğŸ”§ Mode: {mode}")
        print(f"   {description}")
        print(f"   Response length: {len(response.response)} chars")
        print(f"   Preview: {response.response[:120]}...")
        print()


# endregion


# region Demo 4: Retrieval Evaluation


def demo_retrieval_evaluation():
    """demonstrate evaluating retrieval quality"""
    print_section("Demo 4: Retrieval Quality Metrics")

    documents = [
        Document(
            text="Python is a high-level programming language known for readability.",
            metadata={"topic": "python", "relevance": "high"},
        ),
        Document(
            text="JavaScript is essential for web development and runs in browsers.",
            metadata={"topic": "javascript", "relevance": "low"},
        ),
        Document(
            text="Python is widely used in data science and machine learning.",
            metadata={"topic": "python", "relevance": "high"},
        ),
        Document(
            text="TypeScript adds static typing to JavaScript.",
            metadata={"topic": "javascript", "relevance": "low"},
        ),
    ]

    index = VectorStoreIndex.from_documents(documents)

    # query about Python
    query = "What is Python used for?"
    retriever = VectorIndexRetriever(index=index, similarity_top_k=3)

    print(f"ğŸ’¬ Query: {query}\n")
    print("ğŸ” Retrieved chunks:")

    nodes = retriever.retrieve(query)

    for i, node in enumerate(nodes, 1):
        relevance = node.metadata.get("relevance", "unknown")
        score = node.score if hasattr(node, "score") else "N/A"

        print(f"\n  {i}. Relevance: {relevance} | Score: {score}")
        print(f"     Text: {node.text[:80]}...")

    # calculate simple metrics
    high_relevance = sum(1 for node in nodes if node.metadata.get("relevance") == "high")
    precision = high_relevance / len(nodes) if nodes else 0

    print(
        f"\nğŸ“Š Metrics:\n"
        f"   Retrieved: {len(nodes)} chunks\n"
        f"   Relevant: {high_relevance} chunks\n"
        f"   Precision: {precision:.2%}\n"
    )

    print(
        cleandoc("""
        ğŸ’¡ Retrieval Metrics:
        â€¢ Precision: % of retrieved docs that are relevant
        â€¢ Recall: % of relevant docs that were retrieved
        â€¢ MRR (Mean Reciprocal Rank): Quality of top result
        â€¢ NDCG: Ranking quality (graded relevance)

        In production:
        â€¢ Use test sets with known relevant docs
        â€¢ Track metrics over time
        â€¢ A/B test different retrieval configs
    """)
    )


# endregion


# region Demo 5: Context Window Management


def demo_context_window():
    """demonstrate managing context window limits"""
    print_section("Demo 5: Context Window Management")

    # create documents with varying sizes
    documents = [
        Document(text="Short document: LlamaIndex handles context windows automatically."),
        Document(
            text="Medium document: " + " ".join(["Context management is important."] * 10)
        ),
        Document(text="Another short: Vector search retrieves relevant content."),
        Document(text="Long document: " + " ".join(["This is detailed content."] * 50)),
    ]

    print("ğŸ“š Document Sizes:")
    for i, doc in enumerate(documents, 1):
        print(f"   {i}. {len(doc.text)} characters")

    index = VectorStoreIndex.from_documents(documents)

    # demonstrate different context budgets
    configs = [
        {"top_k": 2, "label": "Conservative (top-2)"},
        {"top_k": 4, "label": "Aggressive (all docs)"},
    ]

    query = "Tell me about context management"
    print(f"\nğŸ’¬ Query: {query}\n")

    for config in configs:
        query_engine = index.as_query_engine(similarity_top_k=config["top_k"])

        response = query_engine.query(query)

        total_chars = sum(len(node.text) for node in response.source_nodes)

        print(f"ğŸ”§ {config['label']}")
        print(f"   Retrieved: {len(response.source_nodes)} chunks")
        print(f"   Total context: {total_chars} chars (~{total_chars // 4} tokens)")
        print(f"   Response: {response.response[:100]}...")
        print()

    print(
        cleandoc("""
        ğŸ’¡ Context Window Strategies:
        â€¢ Start conservative, increase if needed
        â€¢ Monitor token usage (prompt + completion)
        â€¢ Use smaller chunks for finer control
        â€¢ Consider compacting/summarizing long chunks
        â€¢ Reserve tokens for response generation

        Typical limits:
        â€¢ GPT-3.5 Turbo: 16K tokens (12K for context)
        â€¢ GPT-4 Turbo: 128K tokens (100K for context)
        â€¢ Claude 3: 200K tokens (180K for context)
        â€¢ Llama 3.1: 128K tokens (100K for context)
    """)
    )


# endregion


# region Demo 6: Production Best Practices


def demo_production_patterns():
    """demonstrate production-ready RAG patterns"""
    print_section("Demo 6: Production RAG Patterns")

    print(
        cleandoc("""
        ğŸ­ Production RAG Checklist:

        1ï¸âƒ£  **Chunking Strategy**
           âœ“ Optimal size: 200-512 tokens per chunk
           âœ“ Overlap: 10-20% to prevent context splits
           âœ“ Sentence-aware splitting (don't break mid-sentence)
           âœ“ Consider semantic chunking for better coherence

        2ï¸âƒ£  **Retrieval Configuration**
           âœ“ Top-K: Start with 3-5, tune based on metrics
           âœ“ Similarity threshold: Filter low-relevance chunks
           âœ“ Metadata filters: Enable faceted search
           âœ“ Reranking: Use cross-encoder for better ordering

        3ï¸âƒ£  **Response Quality**
           âœ“ Synthesis mode: Use 'compact' for efficiency
           âœ“ Streaming: Enable for better UX
           âœ“ Citations: Include source references
           âœ“ Fallback: Handle no-result scenarios gracefully

        4ï¸âƒ£  **Performance Optimization**
           âœ“ Caching: Cache embeddings and frequent queries
           âœ“ Async: Use async for concurrent operations
           âœ“ Batch processing: Embed in batches during indexing
           âœ“ Vector DB: Choose based on scale (local vs cloud)

        5ï¸âƒ£  **Monitoring & Evaluation**
           âœ“ Track retrieval precision/recall
           âœ“ Monitor response latency (p50, p95, p99)
           âœ“ Log failed queries for analysis
           âœ“ A/B test configuration changes
           âœ“ User feedback collection

        6ï¸âƒ£  **Cost Management**
           âœ“ Embedding costs: Use local models or cache
           âœ“ LLM costs: Monitor prompt/completion tokens
           âœ“ Vector DB: Balance performance vs cost
           âœ“ Batch operations: Reduce API calls

        7ï¸âƒ£  **Data Management**
           âœ“ Version control: Track index versions
           âœ“ Incremental updates: Add new docs without full rebuild
           âœ“ Data privacy: Use local models for sensitive data
           âœ“ Backup: Regular vector store backups

        ğŸ“Š Typical Production Architecture:

        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  User Query                         â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Query Transform  â”‚ (optional)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Vector Retrieval â”‚ (top-k chunks)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    Reranking      â”‚ (cross-encoder)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   LLM Synthesis   â”‚ (generate response)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Response + Cites â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

        ğŸ¯ Performance Targets:
        â€¢ Indexing: < 1 sec per document
        â€¢ Retrieval: < 200ms (p95)
        â€¢ End-to-end: < 2 sec (p95)
        â€¢ Accuracy: > 80% user satisfaction

        ğŸ’° Cost Optimization:
        â€¢ Use local embeddings (HuggingFace)
        â€¢ Cache frequent queries
        â€¢ Batch embed during off-peak
        â€¢ Monitor and set budget alerts
    """)
    )


# endregion


# region Main Menu


DEMOS = [
    Demo("1", "Custom Retrieval", "control top-k and parameters", demo_custom_retrieval, needs_api=True),
    Demo("2", "Query Transformation", "improve retrieval quality", demo_query_transformation, needs_api=True),
    Demo("3", "Synthesis Strategies", "different response modes", demo_synthesis_strategies, needs_api=True),
    Demo("4", "Retrieval Evaluation", "measure retrieval quality", demo_retrieval_evaluation, needs_api=True),
    Demo("5", "Context Window", "manage token limits", demo_context_window, needs_api=True),
    Demo("6", "Production Patterns", "best practices and checklist", demo_production_patterns),
]


def main() -> None:
    """run interactive demo menu"""
    has_openai, _ = check_api_keys()

    if not has_openai:
        print("\nâš ï¸  Warning: OPENAI_API_KEY not found")
        print("Most demos require an API key to run.\n")

    runner = MenuRunner(
        DEMOS,
        title="LlamaIndex Advanced RAG",
        subtitle="Production patterns and optimization",
        has_api=has_openai,
    )
    runner.run()


# endregion

if __name__ == "__main__":
    main()
